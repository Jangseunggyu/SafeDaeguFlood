import pandas as pd
df = pd.read_csv("C:\\Users\\USER\\Desktop\\cvs\\êµ­í† êµí†µë¶€_ì €ì¸µì£¼ê±° ì¹¨ìˆ˜í”¼í•´ ì‹œê°í™”_20221201.csv",encoding='euc-kr', sep=None, engine='python')
df.head()
df['ì‹œë„ëª…'].unique()


df = pd.read_csv("C:\\Users\\USER\\Desktop\\cvs\\í–‰ì •ì•ˆì „ë¶€_ì¸ëª…í”¼í•´ ìš°ë ¤ì§€ì—­ í˜„í™©_20240731.csv",encoding='euc-kr', sep=None, engine='python')
df.head()
df.loc[df['ì‹œë„'] == 'ëŒ€êµ¬ê´‘ì—­ì‹œ']


df = pd.read_csv("C:\\Users\\USER\\Desktop\cvs\\ì¸ì²œê´‘ì—­ì‹œ ë‚¨ë™êµ¬_ì¹¨ìˆ˜ìœ„í—˜ì§€ì—­í˜„í™©_20240729.csv",encoding='euc-kr', sep=None, engine='python')

df = pd.read_csv("C:\\Users\\USER\\Desktop\\ëŒ€êµ¬\\daegu_region.csv",encoding='euc-kr', sep=None, engine='python')

df.head()


df = pd.read_csv("C:\\Users\\USER\\Desktop\\cvs\\ëŒ€êµ¬ê´‘ì—­ì‹œ_ì†Œë°© ê¸´ê¸‰êµ¬ì¡° ê¸°ìƒì •ë³´.csv",encoding='euc-kr', sep=None, engine='python')
df[df['ì œëª©'].str.contains("í˜¸ìš°",na=False)]

[df['ì œëª©'] == 'í˜¸ìš°ì£¼ì˜ë³´ ë°œí‘œ']

df['ë°œí‘œì¼ì‹œ'].sort_values(ascending=False)



df = pd.read_csv("C:\\Users\\USER\\Desktop\\ëŒ€êµ¬\\ëŒ€êµ¬ê´‘ì—­ì‹œ_ì„œêµ¬_í•˜ìˆ˜ì‹œì„¤ ì •ë¹„ ê³„íš_20240718.csv",encoding='euc-kr', sep=None, engine='python')
df.info()
df.head()

df = pd.read_csv("C:\\Users\\USER\\Desktop\\cvs\\ëŒ€êµ¬ê´‘ì—­ì‹œ ë¶êµ¬_ì†Œí•˜ì²œ_20240725.csv",encoding='949', sep=None, engine='python')
df
df.head()
df['ì¢…ì ìœ„ì¹˜'].unique()

df = pd.read_csv("C:\\Users\\USER\\Desktop\\ëŒ€êµ¬\\ëŒ€êµ¬ê´‘ì—­ì‹œ ë‹¬ì„±êµ°_ë°°ìˆ˜íŒí”„ì¥_20240829.csv",encoding='949', sep=None, engine='python')
df.head()
df.loc[df['ì„¤ì¹˜ì—°ë„']=='1986']

df = pd.read_csv("C:\\Users\\USER\\Desktop\\ëŒ€êµ¬\\ëŒ€êµ¬ê´‘ì—­ì‹œ_ë¹—ë¬¼íŒí”„ì¥ í˜„í™©_20250409.csv",encoding='949', sep=None, engine='python')

df = pd.read_csv("C:\\Users\\USER\\Desktop\ëŒ€êµ¬\\ëŒ€êµ¬ê´‘ì—­ì‹œ_ë‹¬ì„±êµ°_ì¬í•´ìœ„í—˜ì§€êµ¬_20200702_1593688929259_2221.csv",encoding='euc-kr', sep=None, engine='python')

df = pd.read_csv("C:\\Users\\USER\\Desktop\\SafeDaeguFlood\\ì¬í•´ìœ„í—˜ì§€ì—­\\ëŒ€êµ¬ê´‘ì—­ì‹œ_ìˆ˜ì„±êµ¬_ì¬í•´ìœ„í—˜ì§€êµ¬_20200709_1594518868929_455.csv",encoding='euc-kr', sep=None, engine='python')
df = pd.read_csv("C:\\Users\\USER\\Desktop\\SafeDaeguFlood\\ì¬í•´ìœ„í—˜ì§€ì—­\\ëŒ€êµ¬ê´‘ì—­ì‹œ_ë™êµ¬_ì¬í•´ìœ„í—˜ì§€êµ¬_20200423_1587606070033_618.csv",encoding='euc-kr', sep=None, engine='python')
df = pd.read_csv("C:\\Users\\USER\\Desktop\\SafeDaeguFlood\\ì¬í•´ìœ„í—˜ì§€ì—­\\ëŒ€êµ¬ê´‘ì—­ì‹œ_ë¶êµ¬_ì¬í•´ìœ„í—˜ì§€êµ¬_20200211.csv",encoding='euc-kr', sep=None, engine='python')
df = pd.read_csv("C:\\Users\\USER\\Desktop\\SafeDaeguFlood\\ì¬í•´ìœ„í—˜ì§€ì—­\\ëŒ€êµ¬ê´‘ì—­ì‹œ_ë‹¬ì„±êµ°_ì¬í•´ìœ„í—˜ì§€êµ¬_20200702_1593688929259_2221.csv",encoding='euc-kr', sep=None, engine='python')


import pandas as pd
import folium
from geopy.geocoders import Nominatim
import time

# === 1. CSV í•©ì¹˜ê¸° ===
files = [
    "C:\\Users\\USER\\Desktop\\SafeDaeguFlood\\ì¬í•´ìœ„í—˜ì§€ì—­\\ëŒ€êµ¬ê´‘ì—­ì‹œ_ë™êµ¬_ì¬í•´ìœ„í—˜ì§€êµ¬_20200423_1587606070033_618.csv",
    "C:\\Users\\USER\\Desktop\\SafeDaeguFlood\\ì¬í•´ìœ„í—˜ì§€ì—­\\ëŒ€êµ¬ê´‘ì—­ì‹œ_ë¶êµ¬_ì¬í•´ìœ„í—˜ì§€êµ¬_20200211.csv",
    "C:\\Users\\USER\\Desktop\\SafeDaeguFlood\\ì¬í•´ìœ„í—˜ì§€ì—­\\ëŒ€êµ¬ê´‘ì—­ì‹œ_ìˆ˜ì„±êµ¬_ì¬í•´ìœ„í—˜ì§€êµ¬_20200709_1594518868929_455.csv",
    "C:\\Users\\USER\\Desktop\\SafeDaeguFlood\\ì¬í•´ìœ„í—˜ì§€ì—­\\ëŒ€êµ¬ê´‘ì—­ì‹œ_ë‹¬ì„±êµ°_ì¬í•´ìœ„í—˜ì§€êµ¬_20200702_1593688929259_2221.csv"
]

dfs = []
for file in files:
    df = pd.read_csv(file, encoding="cp949")
    dfs.append(df)

data = pd.concat(dfs, ignore_index=True)

# === 2. ì¹¨ìˆ˜ìœ„í—˜ í•„í„°ë§ ===
# ìœ„í—˜ìœ í˜• ì»¬ëŸ¼ëª…ì´ ë‹¤ë¥´ë©´ ì—¬ê¸°ì„œ print(data.columns)ë¡œ í™•ì¸ í›„ ìˆ˜ì •
flood_data = data[data['ì¬í•´ìœ„í—˜ìœ í˜•'].str.contains('ì¹¨ìˆ˜', na=False)].copy()

# === 3. ì£¼ì†Œ â†’ ì¢Œí‘œ ë³€í™˜ ===
geolocator = Nominatim(user_agent="daegu_flood_mapping")
latitudes = []
longitudes = []

for addr in flood_data['ì¬í•´ìœ„í—˜ì§€ì—­ìƒì„¸ì£¼ì†Œ']:
    try:
        location = geolocator.geocode(f"ëŒ€êµ¬ê´‘ì—­ì‹œ {addr}")
        if location:
            latitudes.append(location.latitude)
            longitudes.append(location.longitude)
        else:
            latitudes.append(None)
            longitudes.append(None)
    except:
        latitudes.append(None)
        longitudes.append(None)
    time.sleep(1)  # ê³¼ë„í•œ ìš”ì²­ ë°©ì§€

flood_data['ìœ„ë„'] = latitudes
flood_data['ê²½ë„'] = longitudes

# === 4. folium ì§€ë„ ìƒì„± ===
flood_data_geo = flood_data.dropna(subset=['ìœ„ë„', 'ê²½ë„'])
m = folium.Map(location=[35.8714, 128.6014], zoom_start=11)

for _, row in flood_data_geo.iterrows():
    folium.Marker(
        location=[row['ìœ„ë„'], row['ê²½ë„']],
        popup=f"{row['ì¬í•´ìœ„í—˜ì§€êµ¬ëª…']}<br>{row['ì¬í•´ìœ„í—˜ì§€ì—­ìƒì„¸ì£¼ì†Œ']}<br>{row['ì¬í•´ìœ„í—˜ì§€ì—­']}",
        icon=folium.Icon(color='red', icon='info-sign')
    ).add_to(m)

# ì €ì¥
m.save("daegu_flood_risk_map.html")
print("âœ… ì§€ë„ ì €ì¥ ì™„ë£Œ: daegu_flood_risk_map.html")


##################################################################################################

import pandas as pd
import folium
from folium.plugins import HeatMap
from geopy.geocoders import Nominatim
import time

# === 1. CSV í•©ì¹˜ê¸° ===
files = [
    "C:\\Users\\USER\\Desktop\\SafeDaeguFlood\\ì¬í•´ìœ„í—˜ì§€ì—­\\ëŒ€êµ¬ê´‘ì—­ì‹œ_ë™êµ¬_ì¬í•´ìœ„í—˜ì§€êµ¬_20200423_1587606070033_618.csv",
    "C:\\Users\\USER\\Desktop\\SafeDaeguFlood\\ì¬í•´ìœ„í—˜ì§€ì—­\\ëŒ€êµ¬ê´‘ì—­ì‹œ_ë¶êµ¬_ì¬í•´ìœ„í—˜ì§€êµ¬_20200211.csv",
    "C:\\Users\\USER\\Desktop\\SafeDaeguFlood\\ì¬í•´ìœ„í—˜ì§€ì—­\\ëŒ€êµ¬ê´‘ì—­ì‹œ_ìˆ˜ì„±êµ¬_ì¬í•´ìœ„í—˜ì§€êµ¬_20200709_1594518868929_455.csv",
    "C:\\Users\\USER\\Desktop\\SafeDaeguFlood\\ì¬í•´ìœ„í—˜ì§€ì—­\\ëŒ€êµ¬ê´‘ì—­ì‹œ_ë‹¬ì„±êµ°_ì¬í•´ìœ„í—˜ì§€êµ¬_20200702_1593688929259_2221.csv"
]

dfs = []
for file in files:
    df = pd.read_csv(file, encoding="cp949")
    dfs.append(df)

data = pd.concat(dfs, ignore_index=True)

# === 2. ì¹¨ìˆ˜ìœ„í—˜ í•„í„°ë§ ===
flood_data = data[data['ì¬í•´ìœ„í—˜ìœ í˜•'].str.contains('ì¹¨ìˆ˜', na=False)].copy()

# === 3. ì£¼ì†Œ â†’ ì¢Œí‘œ ë³€í™˜ ===
geolocator = Nominatim(user_agent="daegu_flood_mapping")
latitudes = []
longitudes = []

for addr in flood_data['ì¬í•´ìœ„í—˜ì§€ì—­ìƒì„¸ì£¼ì†Œ']:
    try:
        location = geolocator.geocode(f"ëŒ€êµ¬ê´‘ì—­ì‹œ {addr}")
        if location:
            latitudes.append(location.latitude)
            longitudes.append(location.longitude)
        else:
            latitudes.append(None)
            longitudes.append(None)
    except:
        latitudes.append(None)
        longitudes.append(None)
    time.sleep(1)  # ìš”ì²­ ì œí•œì„ ìœ„í•´ 1ì´ˆ ì‰¬ê¸°

flood_data['ìœ„ë„'] = latitudes
flood_data['ê²½ë„'] = longitudes

# === 4. ìœ„ë„/ê²½ë„ ìˆëŠ” ë°ì´í„°ë§Œ ì¶”ì¶œ ===
flood_data_geo = flood_data.dropna(subset=['ìœ„ë„', 'ê²½ë„'])

# === 5. folium íˆíŠ¸ë§µ ìƒì„± ===
m = folium.Map(location=[35.8714, 128.6014], zoom_start=11)

# íˆíŠ¸ë§µì— í•„ìš”í•œ ì¢Œí‘œ ë¦¬ìŠ¤íŠ¸ ë§Œë“¤ê¸°
heat_data = [[row['ìœ„ë„'], row['ê²½ë„']] for idx, row in flood_data_geo.iterrows()]

# íˆíŠ¸ë§µ ì¶”ê°€
HeatMap(heat_data, radius=15, blur=10).add_to(m)

# === 6. ì €ì¥ ===
m.save("daegu_flood_heatmap.html")
print("âœ… íˆíŠ¸ë§µ ì €ì¥ ì™„ë£Œ: daegu_flood_heatmap.html")

#####################################################################################

import pandas as pd
import matplotlib.pyplot as plt
from geopy.geocoders import Nominatim
import time

# === 1. CSV í•©ì¹˜ê¸° ===
files = [
    "C:\\Users\\USER\\Desktop\\SafeDaeguFlood\\ì¬í•´ìœ„í—˜ì§€ì—­\\ëŒ€êµ¬ê´‘ì—­ì‹œ_ë™êµ¬_ì¬í•´ìœ„í—˜ì§€êµ¬_20200423_1587606070033_618.csv",
    "C:\\Users\\USER\\Desktop\\SafeDaeguFlood\\ì¬í•´ìœ„í—˜ì§€ì—­\\ëŒ€êµ¬ê´‘ì—­ì‹œ_ë¶êµ¬_ì¬í•´ìœ„í—˜ì§€êµ¬_20200211.csv",
    "C:\\Users\\USER\\Desktop\\SafeDaeguFlood\\ì¬í•´ìœ„í—˜ì§€ì—­\\ëŒ€êµ¬ê´‘ì—­ì‹œ_ìˆ˜ì„±êµ¬_ì¬í•´ìœ„í—˜ì§€êµ¬_20200709_1594518868929_455.csv",
    "C:\\Users\\USER\\Desktop\\SafeDaeguFlood\\ì¬í•´ìœ„í—˜ì§€ì—­\\ëŒ€êµ¬ê´‘ì—­ì‹œ_ë‹¬ì„±êµ°_ì¬í•´ìœ„í—˜ì§€êµ¬_20200702_1593688929259_2221.csv"
]

dfs = []
for file in files:
    df = pd.read_csv(file, encoding="cp949")
    dfs.append(df)

data = pd.concat(dfs, ignore_index=True)

# === 2. ì¹¨ìˆ˜ìœ„í—˜ í•„í„°ë§ ===
flood_data = data[data['ì¬í•´ìœ„í—˜ìœ í˜•'].str.contains('ì¹¨ìˆ˜', na=False)].copy()

# === 3. ì£¼ì†Œ â†’ ì¢Œí‘œ ë³€í™˜ ===
geolocator = Nominatim(user_agent="daegu_flood_mapping")
latitudes = []
longitudes = []

for addr in flood_data['ì¬í•´ìœ„í—˜ì§€ì—­ìƒì„¸ì£¼ì†Œ']:
    try:
        location = geolocator.geocode(f"ëŒ€êµ¬ê´‘ì—­ì‹œ {addr}")
        if location:
            latitudes.append(location.latitude)
            longitudes.append(location.longitude)
        else:
            latitudes.append(None)
            longitudes.append(None)
    except:
        latitudes.append(None)
        longitudes.append(None)
    time.sleep(1)  # ìš”ì²­ ì œí•œ ê³ ë ¤

flood_data['ìœ„ë„'] = latitudes
flood_data['ê²½ë„'] = longitudes

# === 4. ìœ„ë„/ê²½ë„ ìˆëŠ” ë°ì´í„°ë§Œ í•„í„°ë§ ===
flood_data_geo = flood_data.dropna(subset=['ìœ„ë„', 'ê²½ë„'])

# === 5. 2D íˆìŠ¤í† ê·¸ë¨(í—¥ìŠ¤ë¹ˆ) ì‹œê°í™” ===
plt.figure(figsize=(10, 8))
plt.hexbin(
    flood_data_geo['ê²½ë„'],
    flood_data_geo['ìœ„ë„'],
    gridsize=50,
    cmap='Reds',
    mincnt=1
)
plt.colorbar(label='ì¹¨ìˆ˜ìœ„í—˜ì§€ì—­ ê°œìˆ˜')
plt.title("ëŒ€êµ¬ ì¹¨ìˆ˜ìœ„í—˜ì§€ì—­ 2D ë¶„í¬ (Hexbin)")
plt.xlabel("ê²½ë„")
plt.ylabel("ìœ„ë„")
plt.grid(True)
plt.show()

############################################################################################

import pandas as pd
import folium
from geopy.distance import geodesic
from geopy.geocoders import Nominatim
import time

# ===== 1. CSV ë¶ˆëŸ¬ì˜¤ê¸° =====
# ì¹¨ìˆ˜ìœ„í—˜ì§€ì—­
flood_files = [
    r"C:\\Users\\USER\Desktop\\SafeDaeguFlood\\ì¬í•´ìœ„í—˜ì§€ì—­\\ëŒ€êµ¬ê´‘ì—­ì‹œ_ë™êµ¬_ì¬í•´ìœ„í—˜ì§€êµ¬_20200423_1587606070033_618.csv",
    r"C:\\Users\\USER\Desktop\\SafeDaeguFlood\\ì¬í•´ìœ„í—˜ì§€ì—­\\ëŒ€êµ¬ê´‘ì—­ì‹œ_ë¶êµ¬_ì¬í•´ìœ„í—˜ì§€êµ¬_20200211.csv",
    r"C:\\Users\\USER\Desktop\\SafeDaeguFlood\\ì¬í•´ìœ„í—˜ì§€ì—­\\ëŒ€êµ¬ê´‘ì—­ì‹œ_ìˆ˜ì„±êµ¬_ì¬í•´ìœ„í—˜ì§€êµ¬_20200709_1594518868929_455.csv",
    r"C:\\Users\\USER\Desktop\\SafeDaeguFlood\\ì¬í•´ìœ„í—˜ì§€ì—­\\ëŒ€êµ¬ê´‘ì—­ì‹œ_ë‹¬ì„±êµ°_ì¬í•´ìœ„í—˜ì§€êµ¬_20200702_1593688929259_2221.csv"
]
dfs = [pd.read_csv(f, encoding="cp949") for f in flood_files]
flood_data = pd.concat(dfs, ignore_index=True)

# ì¹¨ìˆ˜ìœ„í—˜ë§Œ í•„í„°
flood_data = flood_data[flood_data['ì¬í•´ìœ„í—˜ìœ í˜•'].str.contains('ì¹¨ìˆ˜', na=False)].copy()

# ===== 2. ë°°ìˆ˜íŒí”„ì¥ ë°ì´í„° =====
pump_files = [
    "C:\\Users\\USER\\Desktop\\SafeDaeguFlood\\ë°°ìˆ˜íŒí”„ì¥\\ëŒ€êµ¬ê´‘ì—­ì‹œ ì¤‘êµ¬ ë°°ìˆ˜íŒí”„ì¥ì •ë³´_20201013.csv",
    "C:\\Users\\USER\\Desktop\\SafeDaeguFlood\\ë°°ìˆ˜íŒí”„ì¥\\ëŒ€êµ¬ê´‘ì—­ì‹œ_ë¹—ë¬¼íŒí”„ì¥ í˜„í™©_20250409.csv",
    "C:\\Users\\USER\\Desktop\\SafeDaeguFlood\\ë°°ìˆ˜íŒí”„ì¥\\ëŒ€êµ¬ê´‘ì—­ì‹œ_ì„œêµ¬_ë°°ìˆ˜íŒí”„ì¥ í˜„í™©_20250716.csv",
    "C:\\Users\\USER\\Desktop\\SafeDaeguFlood\\ë°°ìˆ˜íŒí”„ì¥\\ëŒ€êµ¬ê´‘ì—­ì‹œ ë‹¬ì„±êµ°_ë°°ìˆ˜íŒí”„ì¥_20240829.csv"
]
pump_dfs = [pd.read_csv(f, encoding="cp949") for f in pump_files]
pump_data = pd.concat(pump_dfs, ignore_index=True)

# ===== 3. ì£¼ì†Œ â†’ ì¢Œí‘œ ë³€í™˜ =====
geolocator = Nominatim(user_agent="daegu_flood_pump")

def geocode_address(addr):
    try:
        location = geolocator.geocode(f"ëŒ€êµ¬ê´‘ì—­ì‹œ {addr}")
        if location:
            return location.latitude, location.longitude
    except:
        return None, None
    return None, None

# ìœ„í—˜ì§€ì—­ ì¢Œí‘œ
flood_data[['ìœ„ë„', 'ê²½ë„']] = flood_data['ì¬í•´ìœ„í—˜ì§€ì—­ìƒì„¸ì£¼ì†Œ'].apply(
    lambda x: pd.Series(geocode_address(x))
)

# íŒí”„ì¥ ì¢Œí‘œ (ì»¬ëŸ¼ëª… ë§ì¶°ì•¼ í•¨)
pump_address_col = [col for col in pump_data.columns if 'ì£¼ì†Œ' in col][0]
pump_data[['ìœ„ë„', 'ê²½ë„']] = pump_data[pump_address_col].apply(
    lambda x: pd.Series(geocode_address(x))
)

# ===== 4. ê±°ë¦¬ ê³„ì‚° =====
def find_nearest_pump(lat, lon, pump_df):
    if pd.isna(lat) or pd.isna(lon):
        return None, None
    min_dist = float('inf')
    nearest_name = None
    for _, row in pump_df.dropna(subset=['ìœ„ë„','ê²½ë„']).iterrows():
        dist = geodesic((lat, lon), (row['ìœ„ë„'], row['ê²½ë„'])).km
        if dist < min_dist:
            min_dist = dist
            nearest_name = row.get('íŒí”„ì¥ëª…', 'ì´ë¦„ì—†ìŒ')
    return nearest_name, min_dist

nearest_names = []
nearest_dists = []
for _, row in flood_data.iterrows():
    name, dist = find_nearest_pump(row['ìœ„ë„'], row['ê²½ë„'], pump_data)
    nearest_names.append(name)
    nearest_dists.append(dist)

flood_data['ê°€ì¥ê°€ê¹Œìš´íŒí”„ì¥'] = nearest_names
flood_data['íŒí”„ì¥ê±°ë¦¬_km'] = nearest_dists

# ===== 5. ì»¤ë²„ë¦¬ì§€ ë¶„ì„ =====
coverage_threshold = 1.0  # km
outside_coverage = flood_data[flood_data['íŒí”„ì¥ê±°ë¦¬_km'] > coverage_threshold]
print(f"ğŸ“Œ íŒí”„ì¥ {coverage_threshold}km ë°– ìœ„í—˜ì§€ì—­ ìˆ˜: {len(outside_coverage)}")
print(f"ğŸ“Œ ì „ì²´ ëŒ€ë¹„ ë¹„ìœ¨: {len(outside_coverage) / len(flood_data) * 100:.2f}%")

# ===== 6. ì§€ë„ ì‹œê°í™” =====
m = folium.Map(location=[35.8714, 128.6014], zoom_start=11)

# ìœ„í—˜ì§€ì—­ (ë¹¨ê°„ìƒ‰)
for _, row in flood_data.dropna(subset=['ìœ„ë„', 'ê²½ë„']).iterrows():
    folium.CircleMarker(
        location=[row['ìœ„ë„'], row['ê²½ë„']],
        radius=5,
        color='red',
        fill=True,
        fill_opacity=0.7,
        popup=f"ìœ„í—˜ì§€ì—­: {row['ì¬í•´ìœ„í—˜ì§€êµ¬ëª…']}<br>ê°€ê¹Œìš´íŒí”„ì¥: {row['ê°€ì¥ê°€ê¹Œìš´íŒí”„ì¥']} ({row['íŒí”„ì¥ê±°ë¦¬_km']:.2f}km)"
    ).add_to(m)

# íŒí”„ì¥ (íŒŒë€ìƒ‰)
for _, row in pump_data.dropna(subset=['ìœ„ë„', 'ê²½ë„']).iterrows():
    folium.Marker(
        location=[row['ìœ„ë„'], row['ê²½ë„']],
        icon=folium.Icon(color='blue', icon='tint'),
        popup=f"íŒí”„ì¥: {row.get('íŒí”„ì¥ëª…', 'ì´ë¦„ì—†ìŒ')}"
    ).add_to(m)

m.save("daegu_flood_pump_coverage.html")
print("âœ… ì§€ë„ ì €ì¥ ì™„ë£Œ: daegu_flood_pump_coverage.html")

##########################################################################################################
import pandas as pd
import requests
from geopy.distance import geodesic
import matplotlib.pyplot as plt
import seaborn as sns

# ===== 1. ì¹´ì¹´ì˜¤ API ì„¤ì • =====
KAKAO_API_KEY = "ì¹´ì¹´ì˜¤_API_KEY"  # ë°œê¸‰ë°›ì€ í‚¤ ì…ë ¥
headers = {"Authorization": f"KakaoAK {KAKAO_API_KEY}"}

def kakao_geocode(addr):
    try:
        url = "https://dapi.kakao.com/v2/local/search/address.json"
        params = {"query": addr}
        res = requests.get(url, headers=headers, params=params)
        res.raise_for_status()
        documents = res.json().get('documents', [])
        if documents:
            lat = float(documents[0]['y'])
            lon = float(documents[0]['x'])
            return lat, lon
    except:
        return None, None
    return None, None

# ===== 2. ì¹¨ìˆ˜ìœ„í—˜ì§€ì—­ CSV ë¶ˆëŸ¬ì˜¤ê¸° =====
flood_files = [
    r"C:\Users\USER\Desktop\SafeDaeguFlood\ì¬í•´ìœ„í—˜ì§€ì—­\ëŒ€êµ¬ê´‘ì—­ì‹œ_ë™êµ¬_ì¬í•´ìœ„í—˜ì§€êµ¬_20200423_1587606070033_618.csv",
    r"C:\Users\USER\Desktop\SafeDaeguFlood\ì¬í•´ìœ„í—˜ì§€ì—­\ëŒ€êµ¬ê´‘ì—­ì‹œ_ë¶êµ¬_ì¬í•´ìœ„í—˜ì§€êµ¬_20200211.csv",
    r"C:\Users\USER\Desktop\SafeDaeguFlood\ì¬í•´ìœ„í—˜ì§€ì—­\ëŒ€êµ¬ê´‘ì—­ì‹œ_ìˆ˜ì„±êµ¬_ì¬í•´ìœ„í—˜ì§€êµ¬_20200709_1594518868929_455.csv",
    r"C:\Users\USER\Desktop\SafeDaeguFlood\ì¬í•´ìœ„í—˜ì§€ì—­\ëŒ€êµ¬ê´‘ì—­ì‹œ_ë‹¬ì„±êµ°_ì¬í•´ìœ„í—˜ì§€êµ¬_20200702_1593688929259_2221.csv"
]
dfs = [pd.read_csv(f, encoding="cp949") for f in flood_files]
flood_data = pd.concat(dfs, ignore_index=True)
flood_data = flood_data[flood_data['ì¬í•´ìœ„í—˜ìœ í˜•'].str.contains('ì¹¨ìˆ˜', na=False)].copy()

# ì¢Œí‘œ ë³€í™˜
flood_data[['ìœ„ë„', 'ê²½ë„']] = flood_data['ì¬í•´ìœ„í—˜ì§€ì—­ìƒì„¸ì£¼ì†Œ'].apply(
    lambda x: pd.Series(kakao_geocode(f"ëŒ€êµ¬ê´‘ì—­ì‹œ {x}"))
)

# ===== 3. íŒí”„ì¥ CSV ë¶ˆëŸ¬ì˜¤ê¸° =====
pump_files = [
    "C:\\Users\\USER\\Desktop\\SafeDaeguFlood\\ë°°ìˆ˜íŒí”„ì¥\\ëŒ€êµ¬ê´‘ì—­ì‹œ ì¤‘êµ¬ ë°°ìˆ˜íŒí”„ì¥ì •ë³´_20201013.csv",
    "C:\\Users\\USER\\Desktop\\SafeDaeguFlood\\ë°°ìˆ˜íŒí”„ì¥\\ëŒ€êµ¬ê´‘ì—­ì‹œ_ë¹—ë¬¼íŒí”„ì¥ í˜„í™©_20250409.csv",
    "C:\\Users\\USER\\Desktop\\SafeDaeguFlood\\ë°°ìˆ˜íŒí”„ì¥\\ëŒ€êµ¬ê´‘ì—­ì‹œ_ì„œêµ¬_ë°°ìˆ˜íŒí”„ì¥ í˜„í™©_20250716.csv",
    "C:\\Users\\USER\\Desktop\\SafeDaeguFlood\\ë°°ìˆ˜íŒí”„ì¥\\ëŒ€êµ¬ê´‘ì—­ì‹œ ë‹¬ì„±êµ°_ë°°ìˆ˜íŒí”„ì¥_20240829.csv"
]
pump_dfs = [pd.read_csv(f, encoding="cp949") for f in pump_files]
pump_data = pd.concat(pump_dfs, ignore_index=True)

# ì£¼ì†Œ ì»¬ëŸ¼ ì°¾ê¸°
addr_col = [col for col in pump_data.columns if 'ì£¼ì†Œ' in col][0]
pump_data[['ìœ„ë„', 'ê²½ë„']] = pump_data[addr_col].apply(
    lambda x: pd.Series(kakao_geocode(f"ëŒ€êµ¬ê´‘ì—­ì‹œ {x}"))
)

# ===== 4. ê°€ì¥ ê°€ê¹Œìš´ íŒí”„ì¥ ê±°ë¦¬ ê³„ì‚° =====
def find_nearest_pump(lat, lon, pump_df):
    if pd.isna(lat) or pd.isna(lon):
        return None
    min_dist = float('inf')
    for _, row in pump_df.dropna(subset=['ìœ„ë„','ê²½ë„']).iterrows():
        dist = geodesic((lat, lon), (row['ìœ„ë„'], row['ê²½ë„'])).km
        if dist < min_dist:
            min_dist = dist
    return min_dist

flood_data['íŒí”„ì¥ê±°ë¦¬_km'] = flood_data.apply(
    lambda row: find_nearest_pump(row['ìœ„ë„'], row['ê²½ë„'], pump_data), axis=1
)

# ===== 5. ë¶„ì„ìš© ì»¬ëŸ¼ (êµ¬ ì´ë¦„ ì¶”ì¶œ) =====
flood_data['êµ¬'] = flood_data['ì¬í•´ìœ„í—˜ì§€ì—­'].str.split().str[1]
pump_data['êµ¬'] = pump_data[addr_col].str.split().str[1]

# ===== 6. ê·¸ë˜í”„ =====
plt.rc('font', family='Malgun Gothic')  # í•œê¸€í°íŠ¸ ì„¤ì •
sns.set(style="whitegrid")

# (1) êµ¬ë³„ ìœ„í—˜ì§€ì—­ ìˆ˜ vs íŒí”„ì¥ ìˆ˜
gu_stats = pd.merge(
    flood_data['êµ¬'].value_counts().reset_index().rename(columns={'index':'êµ¬', 'êµ¬':'ìœ„í—˜ì§€ì—­_ìˆ˜'}),
    pump_data['êµ¬'].value_counts().reset_index().rename(columns={'index':'êµ¬', 'êµ¬':'íŒí”„ì¥_ìˆ˜'}),
    on='êµ¬', how='outer'
).fillna(0)

gu_stats.plot(x='êµ¬', kind='bar', figsize=(10,6))
plt.title("êµ¬ë³„ ì¹¨ìˆ˜ìœ„í—˜ì§€ì—­ ìˆ˜ vs íŒí”„ì¥ ìˆ˜")
plt.ylabel("ê°œìˆ˜")
plt.xticks(rotation=45)
plt.show()

# (2) íŒí”„ì¥ ê±°ë¦¬ ë¶„í¬
plt.figure(figsize=(8,5))
sns.histplot(flood_data['íŒí”„ì¥ê±°ë¦¬_km'].dropna(), bins=20, kde=True, color='orange')
plt.title("ì¹¨ìˆ˜ìœ„í—˜ì§€ì—­-íŒí”„ì¥ ê±°ë¦¬ ë¶„í¬")
plt.xlabel("ê±°ë¦¬ (km)")
plt.ylabel("ìœ„í—˜ì§€ì—­ ìˆ˜")
plt.show()

# (3) êµ¬ë³„ í‰ê·  íŒí”„ì¥ ê±°ë¦¬ íˆíŠ¸ë§µ
avg_dist_by_gu = flood_data.groupby('êµ¬')['íŒí”„ì¥ê±°ë¦¬_km'].mean().reset_index()
plt.figure(figsize=(8,5))
sns.heatmap(avg_dist_by_gu.pivot_table(index='êµ¬', values='íŒí”„ì¥ê±°ë¦¬_km'),
            annot=True, cmap='Reds', fmt=".2f")
plt.title("êµ¬ë³„ í‰ê·  íŒí”„ì¥ ê±°ë¦¬(km)")
plt.show()

# ===== 7. ì‚¬ê°ì§€ëŒ€ ë¶„ì„ =====
threshold = 1.0  # km
outside_coverage = flood_data[flood_data['íŒí”„ì¥ê±°ë¦¬_km'] > threshold]
print(f"ğŸ“Œ íŒí”„ì¥ {threshold}km ë°– ìœ„í—˜ì§€ì—­ ìˆ˜: {len(outside_coverage)}")
print(f"ğŸ“Œ ì „ì²´ ëŒ€ë¹„ ë¹„ìœ¨: {len(outside_coverage) / len(flood_data) * 100:.2f}%")


##################################################################################################################3

import pandas as pd

# ==============================
# 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
# ==============================

# ì¹¨ìˆ˜ìœ„í—˜ì§€ì—­ CSVë“¤
flood_files = [
    "C:/Users/USER/Desktop/SafeDaeguFlood/ì¬í•´ìœ„í—˜ì§€ì—­/ëŒ€êµ¬ê´‘ì—­ì‹œ_ë™êµ¬_ì¬í•´ìœ„í—˜ì§€êµ¬_20200423_1587606070033_618.csv",
    "C:/Users/USER/Desktop/SafeDaeguFlood/ì¬í•´ìœ„í—˜ì§€ì—­/ëŒ€êµ¬ê´‘ì—­ì‹œ_ë¶êµ¬_ì¬í•´ìœ„í—˜ì§€êµ¬_20200211.csv",
    "C:/Users/USER/Desktop/SafeDaeguFlood/ì¬í•´ìœ„í—˜ì§€ì—­/ëŒ€êµ¬ê´‘ì—­ì‹œ_ìˆ˜ì„±êµ¬_ì¬í•´ìœ„í—˜ì§€êµ¬_20200709_1594518868929_455.csv",
    "C:/Users/USER/Desktop/SafeDaeguFlood/ì¬í•´ìœ„í—˜ì§€ì—­/ëŒ€êµ¬ê´‘ì—­ì‹œ_ë‹¬ì„±êµ°_ì¬í•´ìœ„í—˜ì§€êµ¬_20200702_1593688929259_2221.csv"
]

flood_dfs = [pd.read_csv(f, encoding="cp949") for f in flood_files]
flood_data = pd.concat(flood_dfs, ignore_index=True)

# íŒí”„ì¥ CSVë“¤
pump_files = [
    "C:\\Users\\USER\\Desktop\\SafeDaeguFlood\\ë°°ìˆ˜íŒí”„ì¥\\ëŒ€êµ¬ê´‘ì—­ì‹œ ì¤‘êµ¬ ë°°ìˆ˜íŒí”„ì¥ì •ë³´_20201013.csv",
    "C:\\Users\\USER\\Desktop\\SafeDaeguFlood\\ë°°ìˆ˜íŒí”„ì¥\\ëŒ€êµ¬ê´‘ì—­ì‹œ_ë¹—ë¬¼íŒí”„ì¥ í˜„í™©_20250409.csv",
    "C:\\Users\\USER\\Desktop\\SafeDaeguFlood\\ë°°ìˆ˜íŒí”„ì¥\\ëŒ€êµ¬ê´‘ì—­ì‹œ_ì„œêµ¬_ë°°ìˆ˜íŒí”„ì¥ í˜„í™©_20250716.csv",
    "C:\\Users\\USER\\Desktop\\SafeDaeguFlood\\ë°°ìˆ˜íŒí”„ì¥\\ëŒ€êµ¬ê´‘ì—­ì‹œ ë‹¬ì„±êµ°_ë°°ìˆ˜íŒí”„ì¥_20240829.csv"
]
pump_dfs = [pd.read_csv(f, encoding="cp949") for f in pump_files]
pump_data = pd.concat(pump_dfs, ignore_index=True)

# ì¸êµ¬ë°€ë„
pop_density = pd.read_csv("C:\\Users\\USER\\Desktop\\SafeDaeguFlood\\ë¶„ì„4ì¸êµ¬\\4_ì·¨ì•½ê³„ì¸µ(1)ì¸êµ¬ë°€ë„\\(1)ì¸êµ¬ë°€ë„.csv", encoding="utf-8-sig")

# ì—°ë ¹ë³„ ì¸êµ¬ (ê³ ë ¹ì¸êµ¬ ë¹„ìœ¨ ê³„ì‚°)
age_df = pd.read_csv("C:\\Users\\USER\\Desktop\\SafeDaeguFlood\\ë¶„ì„4ì¸êµ¬\\4_ì·¨ì•½ê³„ì¸µ(2)ì–´ë¦°ì´ë…¸ì¸\\(1)ë™Â·ìÂ·ë©´_ì—°ë ¹ë³„_ì£¼ë¯¼ë“±ë¡ì¸êµ¬_ë‚´êµ­ì¸_ì „ì²´ì—°ë ¹_20250812133656.csv")

# ==============================
# 2. ì „ì²˜ë¦¬
# ==============================

# êµ¬ ì´ë¦„ë§Œ ì¶”ì¶œ
flood_data['êµ¬'] = flood_data['ì¬í•´ìœ„í—˜ì§€ì—­'].str.extract(r'ëŒ€êµ¬ê´‘ì—­ì‹œ\s*(\S+)')
pump_data['êµ¬'] = pump_data['ì£¼ì†Œ'].str.extract(r'ëŒ€êµ¬ê´‘ì—­ì‹œ\s*(\S+)')
pop_density['êµ¬'] = pop_density['í–‰ì •êµ¬'].str.replace(' ', '')

# ê³ ë ¹ì¸êµ¬ ë¹„ìœ¨ ê³„ì‚°
age_df['êµ¬'] = age_df['í–‰ì •êµ¬ì—­'].str.extract(r'ëŒ€êµ¬ê´‘ì—­ì‹œ\s*(\S+)')
age_grouped = age_df.groupby('êµ¬').agg({'65ì„¸ì´ìƒ':'sum','ì´ì¸êµ¬':'sum'}).reset_index()
age_grouped['ê³ ë ¹ì¸êµ¬ë¹„ìœ¨'] = age_grouped['65ì„¸ì´ìƒ'] / age_grouped['ì´ì¸êµ¬'] * 100

# ==============================
# 3. êµ¬ë³„ í†µê³„
# ==============================

flood_count = flood_data.groupby('êµ¬').size().reset_index(name='ì¹¨ìˆ˜ìœ„í—˜ì§€ì—­ìˆ˜')
pump_count = pump_data.groupby('êµ¬').size().reset_index(name='íŒí”„ì¥ìˆ˜')

df = flood_count.merge(pump_count, on='êµ¬', how='outer').fillna(0)
df = df.merge(pop_density[['êµ¬','ì¸êµ¬ë°€ë„']], on='êµ¬', how='left')
df = df.merge(age_grouped[['êµ¬','ê³ ë ¹ì¸êµ¬ë¹„ìœ¨']], on='êµ¬', how='left')

# ==============================
# 4. ì ìˆ˜ ê³„ì‚°
# ==============================

def normalize(series, reverse=False):
    if reverse:  # ê°’ì´ ì‘ì„ìˆ˜ë¡ ì ìˆ˜ ë†’ìŒ
        return (series.max() - series) / (series.max() - series.min()) * 100
    else:
        return (series - series.min()) / (series.max() - series.min()) * 100

df['ìœ„í—˜ì§€ì—­ì ìˆ˜'] = normalize(df['ì¹¨ìˆ˜ìœ„í—˜ì§€ì—­ìˆ˜'])
df['íŒí”„ì¥ë¶€ì¡±ì ìˆ˜'] = normalize(df['íŒí”„ì¥ìˆ˜'], reverse=True)
df['ì¸êµ¬ë°€ë„ì ìˆ˜'] = normalize(df['ì¸êµ¬ë°€ë„'])
df['ê³ ë ¹ì¸êµ¬ì ìˆ˜'] = normalize(df['ê³ ë ¹ì¸êµ¬ë¹„ìœ¨'])

# ê°€ì¤‘ì¹˜ ì ìš©
df['ìœ„í—˜ì ìˆ˜'] = (
    df['ìœ„í—˜ì§€ì—­ì ìˆ˜'] * 0.3 +
    df['íŒí”„ì¥ë¶€ì¡±ì ìˆ˜'] * 0.2 +
    df['ì¸êµ¬ë°€ë„ì ìˆ˜'] * 0.3 +
    df['ê³ ë ¹ì¸êµ¬ì ìˆ˜'] * 0.2
)

# ==============================
# 5. ì €ì¥
# ==============================
df = df[['êµ¬','ì¹¨ìˆ˜ìœ„í—˜ì§€ì—­ìˆ˜','íŒí”„ì¥ìˆ˜','ì¸êµ¬ë°€ë„','ê³ ë ¹ì¸êµ¬ë¹„ìœ¨','ìœ„í—˜ì ìˆ˜']]
df.to_csv("ìœ„í—˜ì ìˆ˜_ê¸°ì¤€í‘œ.csv", index=False, encoding="utf-8-sig")

print("âœ… ìœ„í—˜ ì ìˆ˜ í‘œ ì €ì¥ ì™„ë£Œ: ìœ„í—˜ì ìˆ˜_ê¸°ì¤€í‘œ.csv")
print(df.sort_values('ìœ„í—˜ì ìˆ˜', ascending=False))
print(pop_density.columns)
print(age_df.columns)

df = pd.read_csv("C:\\Users\\USER\\Desktop\\SafeDaeguFlood\\ë°°ìˆ˜íŒí”„ì¥\\ëŒ€êµ¬ê´‘ì—­ì‹œ_ë¹—ë¬¼íŒí”„ì¥ í˜„í™©_20250409.csv",encoding='949')

df.head()

################################################################################################################################
import pandas as pd
import glob

# 1ï¸âƒ£ ë°°ìˆ˜íŒí”„ì¥ íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
pump_files = [
    r"C:\Users\USER\Desktop\SafeDaeguFlood\ë°°ìˆ˜íŒí”„ì¥\ëŒ€êµ¬ê´‘ì—­ì‹œ_ì„œêµ¬_ë°°ìˆ˜íŒí”„ì¥ í˜„í™©_20250716.csv",
    r"C:\Users\USER\Desktop\SafeDaeguFlood\ë°°ìˆ˜íŒí”„ì¥\ëŒ€êµ¬ê´‘ì—­ì‹œ_ë¹—ë¬¼íŒí”„ì¥ í˜„í™©_20250409.csv",
    r"C:\Users\USER\Desktop\SafeDaeguFlood\ë°°ìˆ˜íŒí”„ì¥\ëŒ€êµ¬ê´‘ì—­ì‹œ ì¤‘êµ¬ ë°°ìˆ˜íŒí”„ì¥ì •ë³´_20201013.csv",
    r"C:\Users\USER\Desktop\SafeDaeguFlood\ë°°ìˆ˜íŒí”„ì¥\ëŒ€êµ¬ê´‘ì—­ì‹œ ë‹¬ì„±êµ°_ë°°ìˆ˜íŒí”„ì¥_20240829.csv"
]

pump_list = []
for f in pump_files:
    df = pd.read_csv(f, encoding='cp949')  # í˜¹ì‹œ ì¸ì½”ë”© ë¬¸ì œ ì‹œ 'euc-kr'ë„ ê°€ëŠ¥
    df.columns = df.columns.str.strip()  # ê³µë°± ì œê±°
    pump_list.append(df)

pump_df = pd.concat(pump_list, ignore_index=True)

# ë°°ìˆ˜íŒí”„ì¥ ë°ì´í„° í™•ì¸ í›„ 'êµ¬' ì»¬ëŸ¼ê³¼ 'ìš©ëŸ‰(HP)' ì»¬ëŸ¼ ì¶”ì¶œ
# ì‹¤ì œ ì»¬ëŸ¼ëª…ì— ë§ê²Œ ìˆ˜ì • í•„ìš”
pump_df['êµ¬'] = pump_df['í–‰ì •êµ¬ì—­'].str.replace(' ', '')
pump_df['ë°°ìˆ˜ìš©ëŸ‰'] = pump_df['íŒí”„']  # ì»¬ëŸ¼ëª…ì— ë§ê²Œ ìˆ˜ì •
pump_df_grouped = pump_df.groupby('êµ¬')['ë°°ìˆ˜ìš©ëŸ‰'].sum().reset_index()

# 2ï¸âƒ£ ì¬í•´ìœ„í—˜ì§€ì—­ íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
hazard_files = [
    r"C:\Users\USER\Desktop\SafeDaeguFlood\ì¬í•´ìœ„í—˜ì§€ì—­\ëŒ€êµ¬ê´‘ì—­ì‹œ_ìˆ˜ì„±êµ¬_ì¬í•´ìœ„í—˜ì§€êµ¬_20200709_1594518868929_455.csv",
    r"C:\Users\USER\Desktop\SafeDaeguFlood\ì¬í•´ìœ„í—˜ì§€ì—­\ëŒ€êµ¬ê´‘ì—­ì‹œ_ë¶êµ¬_ì¬í•´ìœ„í—˜ì§€êµ¬_20200211.csv",
    r"C:\Users\USER\Desktop\SafeDaeguFlood\ì¬í•´ìœ„í—˜ì§€ì—­\ëŒ€êµ¬ê´‘ì—­ì‹œ_ë™êµ¬_ì¬í•´ìœ„í—˜ì§€êµ¬_20200423_1587606070033_618.csv",
    r"C:\Users\USER\Desktop\SafeDaeguFlood\ì¬í•´ìœ„í—˜ì§€ì—­\ëŒ€êµ¬ê´‘ì—­ì‹œ_ë‹¬ì„±êµ°_ì¬í•´ìœ„í—˜ì§€êµ¬_20200702_1593688929259_2221.csv"
]

hazard_list = []
for f in hazard_files:
    df = pd.read_csv(f, encoding='cp949')
    df.columns = df.columns.str.strip()
    hazard_list.append(df)

hazard_df = pd.concat(hazard_list, ignore_index=True)

# 'êµ¬' ì»¬ëŸ¼ê³¼ 'ì¬í•´ìœ„í—˜ì§€ì—­ë©´ì ' ì¶”ì¶œ (ì»¬ëŸ¼ëª… í™•ì¸ í•„ìš”)
hazard_df['êµ¬'] = hazard_df['í–‰ì •êµ¬ì—­(êµ¬)'].str.replace(' ', '')
hazard_df['ì¬í•´ìœ„í—˜ì§€ì—­ë©´ì '] = hazard_df['ë©´ì (ã¡)']  # ì»¬ëŸ¼ëª… í™•ì¸ í•„ìš”
hazard_df_grouped = hazard_df.groupby('êµ¬')['ì¬í•´ìœ„í—˜ì§€ì—­ë©´ì '].sum().reset_index()

# 3ï¸âƒ£ ë°°ìˆ˜íŒí”„ì¥ + ì¬í•´ìœ„í—˜ì§€ì—­ ë³‘í•©
df = pd.merge(hazard_df_grouped, pump_df_grouped, on='êµ¬', how='left')
df['ìš©ëŸ‰_HP'] = df['ìš©ëŸ‰_HP'].fillna(0)  # íŒí”„ ì—†ëŠ” êµ¬ëŠ” 0ìœ¼ë¡œ ì²˜ë¦¬

# 4ï¸âƒ£ ì ìˆ˜ ê³„ì‚°
# ìœ„í—˜ë©´ì  ë¹„ìœ¨
df['ìœ„í—˜ë©´ì ë¹„ìœ¨'] = df['ì¬í•´ìœ„í—˜ì§€ì—­ë©´ì '] / df['ì¬í•´ìœ„í—˜ì§€ì—­ë©´ì '].sum()

# íŒí”„ ìš©ëŸ‰ ì ìˆ˜í™” (ìš©ëŸ‰ ì‘ì„ìˆ˜ë¡ ì ìˆ˜ â†‘)
max_capacity = df['ìš©ëŸ‰_HP'].max()
df['íŒí”„ì¥ì ìˆ˜'] = 1 - (df['ìš©ëŸ‰_HP'] / max_capacity)

# ìµœì¢… ì¹¨ìˆ˜ì·¨ì•½ì ìˆ˜ (ê°€ì¤‘ì¹˜: ìœ„í—˜ë©´ì  0.7, íŒí”„ìš©ëŸ‰ 0.3)
df['ì¹¨ìˆ˜ì·¨ì•½ì ìˆ˜'] = df['ìœ„í—˜ë©´ì ë¹„ìœ¨']*0.7 + df['íŒí”„ì¥ì ìˆ˜']*0.3

# ë“±ê¸‰ ë¶€ì—¬
def assign_grade(score):
    if score < 0.2:
        return 'A'
    elif score < 0.4:
        return 'B'
    elif score < 0.6:
        return 'C'
    elif score < 0.8:
        return 'D'
    else:
        return 'E'

df['ìœ„í—˜ë“±ê¸‰'] = df['ì¹¨ìˆ˜ì·¨ì•½ì ìˆ˜'].apply(assign_grade)

# 5ï¸âƒ£ ê²°ê³¼ í™•ì¸
print(df[['êµ¬','ì¬í•´ìœ„í—˜ì§€ì—­ë©´ì ','ìš©ëŸ‰_HP','ì¹¨ìˆ˜ì·¨ì•½ì ìˆ˜','ìœ„í—˜ë“±ê¸‰']])
