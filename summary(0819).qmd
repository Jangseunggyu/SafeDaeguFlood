---
title: "Safe Daegu Flood"
author: "team 7"
format: dashboard
---

```{python}

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from scipy.stats import uniform
from scipy.stats import norm
from scipy.stats import binom
import scipy.stats as sp
from scipy.stats import t
import scipy.stats as stats
from scipy import stats

import math
from collections import Counter
from scipy.integrate import quad
from scipy.stats import uniform, norm, binom, poisson, expon, gamma, t, chi2, f, beta
from scipy.stats import bernoulli
from scipy.stats import ttest_rel

plt.rcParams['font.family'] = 'Malgun Gothic'
plt.rcParams['axes.unicode_minus'] = False

import plotly.express as px
import plotly.graph_objects as go
import geopandas as gpd
import json 
import folium

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()


# -*- coding: utf-8 -*-
import math
import pandas as pd
from pathlib import Path

# csvë¡œ ì €ì¥í•˜ê¸°
# .to_csv("íŒŒì¼ì´ë¦„.csv", index=False, encoding="utf-8-sig")

```

# \[ì„œë¡ \]

## ìµœê·¼ ëŒ€êµ¬ ì¹¨ìˆ˜ í”¼í•´ ë° ê´€ë ¨ ë¹„ëŒ€ìœ„

```{python}
# ê´€ë ¨ ê¸°ì‚¬ ì²¨ë¶€ ì˜ˆì •

```

# \[ë³¸ë¡ \]

## ê° ìš”ì¸ë³„ ë¶„ì„

# 1. ë‚ ì”¨ ìš”ì¸

## ëŒ€êµ¬ì‹œ ì¥ë§ˆ ê¸°ê°„ ë° ê¸°ê°„ ë‚´ ì´ ê°•ìˆ˜ëŸ‰ ë¶„ì„ (2011\~2024)

```{python}
# ëŒ€êµ¬ì‹œ ì „ì²´ ì¥ë§ˆ ê¸°ê°„ ë° ê¸°ê°„ ë‚´ ì´ ê°•ìˆ˜ëŸ‰
rain = pd.read_csv('./data/rain.csv')
rain.info()
rain

```

## ëŒ€êµ¬ì‹œ í–‰ì •êµ¬ë³„ ê°•ìˆ˜ëŸ‰ (2016.01.01\~2025.08.18)

```{python}

rain = pd.read_csv("./data/daegu_rain10years.csv", encoding="cp949" )
# print(df)

# df.info()
# df.describe()
# df.shape

# =========================================================
# 0) ì‚¬ìš©ì ì…ë ¥: ë„¤ê°€ ê°€ì§„ ê°•ìˆ˜ CSV ê²½ë¡œ ì§€ì •
#    (ì»¬ëŸ¼: ì§€ì , ì§€ì ëª…, ì¼ì‹œ, ì¼ê°•ìˆ˜ëŸ‰(mm))
# =========================================================
# YOUR_RAIN_CSV = "daegu_rain10years.csv"   # <-- íŒŒì¼ëª…/ê²½ë¡œë§Œ ë°”ê¿”ì¤˜

# =========================================================
# 1) ê´€ì¸¡ì†Œ ì¢Œí‘œ CSV ì§ì ‘ ìƒì„±
#    - 845/846/860/828: ê¸°ìƒì²­ ì—°ë³´ í‘œì˜ ë„Â·ë¶„ â†’ 10ì§„ìˆ˜ ë³€í™˜ê°’
#    - 991/992: ë²•ì •ë¦¬ ëŒ€í‘œ ì¢Œí‘œ(ê·¼ì ‘ ëŒ€ì²´ê°’)
# =========================================================
stations = [
    # station_id, name, lat, lon, source_note
    (828, "ë‹¬ì„±",   35 + 41/60, 128 + 25/60, "KMA ì—°ë³´(ë„ë¶„)"),
    (845, "ëŒ€êµ¬ë¶êµ¬",35 + 54/60, 128 + 35/60, "KMA ì—°ë³´(ë„ë¶„)"),
    (846, "ëŒ€êµ¬ì„œêµ¬",35 + 51/60, 128 + 31/60, "KMA ì—°ë³´(ë„ë¶„)"),
    (860, "ì‹ ì•”",   35 + 53/60, 128 + 37/60, "KMA ì—°ë³´(ë„ë¶„)"),
    # ì•„ë˜ ë‘ ê°œëŠ” ì„¤ì¹˜ í–‰ì •ë™ ëŒ€í‘œ ì¢Œí‘œ(ê·¼ì ‘ì¹˜)
    (991, "ì˜¥í¬",   35.794916, 128.440565, "ì˜¥í¬ì ì‹ ë‹¹ë¦¬ ëŒ€í‘œì (ì£¼ì†Œâ†’ì¢Œí‘œ)"),
    (992, "í•˜ë¹ˆ",   35.900780, 128.446059, "í•˜ë¹ˆë©´ í˜„ë‚´ë¦¬(ë©´ë¯¼ìš´ë™ì¥ ì¢Œí‘œ)"),
]
df_stn = pd.DataFrame(stations, columns=["ì§€ì ","ì§€ì ëª…","lat","lon","ì¢Œí‘œì¶œì²˜"])
# df_stn.to_csv("stations_daegu.csv", index=False, encoding="utf-8-sig")

# =========================================================
# 2) ëŒ€êµ¬ 9ê°œ êµ¬Â·êµ° ì¤‘ì‹¬ ì¢Œí‘œ CSV ì§ì ‘ ìƒì„± (ê³µê°œ JSONì˜ ëŒ€í‘œì  ì‚¬ìš©)
# =========================================================
districts = [
    ("ì¤‘êµ¬",   35.86678, 128.59538),
    ("ë™êµ¬",   35.88566, 128.63296),
    ("ì„œêµ¬",   35.87465, 128.55109),
    ("ë‚¨êµ¬",   35.84119, 128.58800),
    ("ë¶êµ¬",   35.90000, 128.59175),
    ("ìˆ˜ì„±êµ¬", 35.85905, 128.62625),
    ("ë‹¬ì„œêµ¬", 35.82569, 128.52403),
    ("ë‹¬ì„±êµ°", 35.77467, 128.42955),
    ("êµ°ìœ„êµ°", 36.16995, 128.64705),  # 2023-07-01 ëŒ€êµ¬ í¸ì…
]
df_ctr = pd.DataFrame(districts, columns=["êµ¬êµ°","lat","lon"])
# df_ctr.to_csv("daegu_district_centroids.csv", index=False, encoding="utf-8-sig")

# =========================================================
# 3) ê°•ìˆ˜ ë°ì´í„° ì½ê¸°
#    - ì¸ì½”ë”© ì´ìŠˆ ìˆì„ ìˆ˜ ìˆì–´ cp949ë„ ì‹œë„
# =========================================================
def read_csv_any(path):
    try:
        return pd.read_csv(path)
    except UnicodeDecodeError:
        return pd.read_csv(path, encoding="cp949")

# rain = read_csv_any(YOUR_RAIN_CSV).copy()

# ì»¬ëŸ¼ í‘œì¤€í™”
rain = rain.rename(columns={"ì¼ê°•ìˆ˜ëŸ‰(mm)":"ì¼ê°•ìˆ˜ëŸ‰"})
rain["ì¼ì‹œ"] = pd.to_datetime(rain["ì¼ì‹œ"]).dt.date  # ë‚ ì§œë§Œ

# =========================================================
# 4) Haversine ê±°ë¦¬(km)
# =========================================================
def haversine_km(lat1, lon1, lat2, lon2):
    R = 6371.0088
    p1, p2 = math.radians(lat1), math.radians(lat2)
    dphi = math.radians(lat2 - lat1)
    dlmb = math.radians(lon2 - lon1)
    a = math.sin(dphi/2)**2 + math.cos(p1)*math.cos(p2)*math.sin(dlmb/2)**2
    return 2*R*math.asin(math.sqrt(a))

# =========================================================
# 5) IDW í•¨ìˆ˜ (p=2, k-ìµœê·¼ì ‘ ì‚¬ìš© ê¶Œì¥)
# =========================================================
def idw_estimate(target_lat, target_lon, stations_df, values_series, p=2, k=4):
    # stations_df: (ì§€ì , lat, lon)
    # values_series: index=ì§€ì , value=í•´ë‹¹ ì§€ì ì˜ ê°’
    rows = []
    for _, r in stations_df.iterrows():
        sid, slat, slon = r["ì§€ì "], r["lat"], r["lon"]
        val = values_series.get(sid, None)
        if pd.notnull(val):
            d = haversine_km(target_lat, target_lon, slat, slon)
            rows.append((sid, d, val))
    if not rows:
        return float("nan")

    # 0ê±°ë¦¬(ê°™ì€ ì ) ë³´í˜¸
    rows = [(sid, max(d, 1e-6), val) for sid, d, val in rows]

    # k-ìµœê·¼ì ‘ë§Œ ì‚¬ìš©(ë„ˆë¬´ ë¨¼ ì§€ì  ì˜í–¥ ì œê±°)
    rows.sort(key=lambda x: x[1])
    rows = rows[:k]

    weights = [1/(d**p) for _, d, _ in rows]
    wsum = sum(weights)
    return sum(w*v for w, (_, _, v) in zip(weights, rows)) / wsum if wsum > 0 else float("nan")

# =========================================================
# 6) ë‚ ì§œë³„ë¡œ ê´€ì¸¡ì†Œ ê°’ â†’ ê° êµ¬Â·êµ° IDW ì¶”ì •
# =========================================================
# ê´€ì¸¡ì†Œ ì¢Œí‘œ í…Œì´ë¸”
stn_xy = df_stn[["ì§€ì ","lat","lon"]].drop_duplicates().set_index("ì§€ì ")

# ì¼ì Ã— ì§€ì  í”¼ë²— (ê°’=ì¼ê°•ìˆ˜ëŸ‰)
pv = rain.pivot_table(index="ì¼ì‹œ", columns="ì§€ì ", values="ì¼ê°•ìˆ˜ëŸ‰", aggfunc="mean")

# ê²°ê³¼ ë‹´ì„ ë¦¬ìŠ¤íŠ¸
out = []
for the_date, row in pv.iterrows():
    # row: index=ì§€ì , value=ê·¸ ë‚  ì§€ì  ê°’
    for _, g in df_ctr.iterrows():
        gname, glat, glon = g["êµ¬êµ°"], g["lat"], g["lon"]
        est = idw_estimate(glat, glon, df_stn[["ì§€ì ","lat","lon"]], row, p=2, k=4)
        out.append([the_date, gname, est])

df_out = pd.DataFrame(out, columns=["ì¼ì‹œ","êµ¬êµ°","ì¶”ì •_ì¼ê°•ìˆ˜ëŸ‰(mm)"])
df_out = df_out.sort_values(["ì¼ì‹œ","êµ¬êµ°"]).reset_index(drop=True)

# ì €ì¥
# df_out.to_csv("ëŒ€êµ¬ í–‰ì •êµ¬ë³„ ê°•ìˆ˜ëŸ‰.csv", index=False, encoding="utf-8-sig")

# ë¯¸ë¦¬ë³´ê¸°(ì•ë¶€ë¶„)
print(df_out)
print("\n== íŒŒì¼ ìƒì„± ì™„ë£Œ ==")
print(" - ê´€ì¸¡ì†Œ ì¢Œí‘œ: stations_daegu.csv")
print(" - êµ¬êµ° ì¤‘ì‹¬:   daegu_district_centroids.csv")
print(" - ê²°ê³¼:        daegu_district_rain_idw.csv")


# df_out.groupby("êµ¬êµ°")['ì¶”ì •_ì¼ê°•ìˆ˜ëŸ‰(mm)'].mean()
# df_out.groupby("êµ¬êµ°")['ì¶”ì •_ì¼ê°•ìˆ˜ëŸ‰(mm)'].describe()

# ì¶œë ¥

rain_gu_mean = df_out.groupby("êµ¬êµ°")['ì¶”ì •_ì¼ê°•ìˆ˜ëŸ‰(mm)'].mean()
# rain_gu_mean
# rain_gu_mean.to_csv("ëŒ€êµ¬ í–‰ì •êµ¬ë³„ ê°•ìˆ˜ëŸ‰ mean.csv", index=False, encoding="utf-8-sig")
rain_gu_describe = df_out.groupby("êµ¬êµ°")['ì¶”ì •_ì¼ê°•ìˆ˜ëŸ‰(mm)'].describe()
# rain_gu_describe
# rain_gu_describe.to_csv("ëŒ€êµ¬ í–‰ì •êµ¬ë³„ ê°•ìˆ˜ëŸ‰ describe.csv", index=False, encoding="utf-8-sig")

```

## í–‰ì •êµ¬ë³„ ì¥ë§ˆ ë‚´ ê°•ìˆ˜ëŸ‰ ë¶„ì„ (2016\~2024)

```{python}
# ì¥ë§ˆê¸°ê°„ ë‚´ í–‰ì •êµ¬ë³„ ì—°ê°„ ê°•ìˆ˜ëŸ‰ ë¶„ì„

# ì¶œë ¥

rainy_gu = pd.read_csv('./data/ì¥ë§ˆê¸°ê°„ í–‰ì •êµ¬ë³„ ê°•ìˆ˜ëŸ‰.csv')
# rainy_gu

# rainy_dong = pd.read_csv('./data/ì¥ë§ˆê¸°ê°„ í–‰ì •êµ¬ë³„ ê°•ìˆ˜ëŸ‰_í–‰ì •ë™ì¶”ê°€.csv')


```

```{python}
# mapping

# ì‹œê°í™”:
# ì§€ì—­ë³„ í‰ê·  or ì§€ì—­ë³„ ê°•ìˆ˜ëŸ‰ ìµœëŒ“ê°’

```

# 2. ì¸êµ¬ ìš”ì¸

## (1) ì „ì²´ ì¸êµ¬ ë°€ë„: â‘  í–‰ì •êµ¬ â‘¡ ìë©´ë™

```{python}
# (1) 4_1_dens ############################################################

# ì „ì²˜ë¦¬

# í–‰ì •êµ¬ì—­ë³„ ì¸êµ¬ë°€ë„
# https://www.daegu.go.kr/index.do?menu_id=00000253

dens = pd.read_csv('./data/4_1_dens.csv')
dens.info()
dens.head()

dens = dens[dens['ìë©´ë™'] != 'ì†Œê³„']
# len(dens['í–‰ì •êµ¬'].unique())
```

```{python}

# (1) ìë©´ë™ ë‹¨ìœ„ ì¸êµ¬ë°€ë„
dens['ì¸êµ¬ë°€ë„'] = (dens['ì¸êµ¬(ëª…)'] / dens['ë©´ì (ã¢)']).round(2)



# (2) í–‰ì •êµ¬ ë‹¨ìœ„: ê° í–‰ì •êµ¬ì˜ (ì´ ì¸êµ¬ / ì´ ë©´ì ) ê¸°ì¤€ ì¸êµ¬ë°€ë„
gu_dens = dens[['í–‰ì •êµ¬','ì¸êµ¬(ëª…)','ë©´ì (ã¢)']].groupby(['í–‰ì •êµ¬'])

gu_dens_sum = gu_dens.sum().reset_index().rename(columns={'index': 'í–‰ì •êµ¬'})
gu_dens_sum['ì¸êµ¬ë°€ë„'] = (gu_dens_sum['ì¸êµ¬(ëª…)'] / gu_dens_sum['ë©´ì (ã¢)']).round(2)



## ì¶œë ¥

# í–‰ì •êµ¬ ë‹¨ìœ„ ì¸êµ¬ë°€ë„
gu_dens_sum
# gu_dens_sum.to_csv("í–‰ì •êµ¬ ì „ì²´ ì¸êµ¬ë°€ë„.csv", index=False, encoding="utf-8-sig")

# ìë©´ë™ ë‹¨ìœ„ ì¸êµ¬ë°€ë„
dens
# dens.to_csv("ìë©´ë™ ì „ì²´ ì¸êµ¬ë°€ë„.csv", index=False, encoding="utf-8-sig")

```

```{python}

# (3) sort -> ê·¸ë˜í”„ ì‹œê°í™”

## ëŒ€êµ¬ ì „ì²´ ìë©´ë™ ì¸êµ¬ë°€ë„
scaler = MinMaxScaler()
dens['ìë©´ë™ ì¸êµ¬ë°€ë„_norm'] = scaler.fit_transform(dens[['ì¸êµ¬ë°€ë„']])
# ìë©´ë™ ì¸êµ¬ë°€ë„ ìƒìœ„ 10
dens_sort = dens[['í–‰ì •êµ¬','ìë©´ë™','ì¸êµ¬(ëª…)','ë©´ì (ã¢)','ì¸êµ¬ë°€ë„','ìë©´ë™ ì¸êµ¬ë°€ë„_norm']]
dens_sort.sort_values(['ì¸êµ¬ë°€ë„'], ascending=False).head(10)
# ìë©´ë™ ì¸êµ¬ë°€ë„ í•˜ìœ„ 10
# dens_sort.sort_values(['ì¸êµ¬ë°€ë„'], ascending=True).head(10)


## í–‰ì •êµ¬ ì¸êµ¬ë°€ë„ ë‚´ë¦¼ì°¨ìˆœ
gu_dens_sum.sort_values(['ì¸êµ¬ë°€ë„'], ascending=False)



## ì¶œë ¥

## í–‰ì •êµ¬ ì¸êµ¬ë°€ë„ ë‚´ë¦¼ì°¨ìˆœ
gu_dens_sum.sort_values(['ì¸êµ¬ë°€ë„'], ascending=False)

# ìë©´ë™ ì¸êµ¬ë°€ë„ ìƒìœ„ 10
dens_sort.sort_values(['ì¸êµ¬ë°€ë„'], ascending=False).head(10)

```

```{python}

# (4) norm í™œìš© -> ì§€ë„ íˆíŠ¸ë§µ?

## í–‰ì •êµ¬ ë‹¨ìœ„
gu_dens_sum['í–‰ì •êµ¬ ì¸êµ¬ë°€ë„_norm'] = scaler.fit_transform(gu_dens_sum[['ì¸êµ¬ë°€ë„']])


## ìë©´ë™ ë‹¨ìœ„

# êµ¬ë³„ ë¦¬ìŠ¤íŠ¸
gu_list = dens_sort['í–‰ì •êµ¬'].unique()

# êµ¬ë³„ DataFrame ì €ì¥ ë”•ì…”ë„ˆë¦¬
gu_dfs = {}

for gu in gu_list:
    gu_df = dens_sort[dens_sort['í–‰ì •êµ¬'] == gu].copy()
    
    # ì¸êµ¬ë°€ë„ ê³„ì‚° (NaN ë°©ì§€)
    gu_df['ì¸êµ¬ë°€ë„'] = (gu_df['ì¸êµ¬(ëª…)'] / gu_df['ë©´ì (ã¢)']).round(2)
    gu_df['ì¸êµ¬ë°€ë„'] = gu_df['ì¸êµ¬ë°€ë„'].fillna(0)
    
    # ì •ê·œí™” (2D array í˜•íƒœë¡œ ë³€í™˜)
    gu_df[f'{gu} ì¸êµ¬ë°€ë„_norm'] = scaler.fit_transform(gu_df[['ì¸êµ¬ë°€ë„']])
    
    gu_dfs[gu] = gu_df



## ì¶œë ¥

## í–‰ì •êµ¬ ë‹¨ìœ„
gu_dens_sum

## ìë©´ë™ ë‹¨ìœ„

# Jung_gu =
gu_dfs['ì¤‘êµ¬'].sort_values(['ì¸êµ¬ë°€ë„'], ascending=False)
# Dong_gu =
gu_dfs['ë™êµ¬'].sort_values(['ì¸êµ¬ë°€ë„'], ascending=False)
# Seo_gu =
gu_dfs['ì„œêµ¬'].sort_values(['ì¸êµ¬ë°€ë„'], ascending=False)
# Nam_gu =
gu_dfs['ë‚¨êµ¬'].sort_values(['ì¸êµ¬ë°€ë„'], ascending=False)
# Buk_gu =
gu_dfs['ë¶êµ¬'].sort_values(['ì¸êµ¬ë°€ë„'], ascending=False)
# Suseong_gu =
gu_dfs['ìˆ˜ì„±êµ¬'].sort_values(['ì¸êµ¬ë°€ë„'], ascending=False)
# Dalseo_gu =
gu_dfs['ë‹¬ì„œêµ¬'].sort_values(['ì¸êµ¬ë°€ë„'], ascending=False)
# Dalseong_gun =
gu_dfs['ë‹¬ì„±êµ°'].sort_values(['ì¸êµ¬ë°€ë„'], ascending=False)
# Gunwi_gun =
gu_dfs['êµ°ìœ„êµ°'].sort_values(['ì¸êµ¬ë°€ë„'], ascending=False)

```

```{python}
# ì§€ë„ ì‹œê°í™”

# --- 1. ë°ì´í„° ë¡œë“œ ---
# CSV íŒŒì¼ì˜ ì •í™•í•œ 9ê°œ ì»¬ëŸ¼ ì´ë¦„ ëª©ë¡
csv_columns = ['í–‰ì •êµ¬', 'ìë©´ë™', 'ì¸êµ¬(ëª…)', 'ë©´ì (ã¢)', 'ì„¸ëŒ€ìˆ˜', 'í†µ', 'ë¦¬', 'ë°˜', 'ì¸êµ¬ë°€ë„']

# usecols ì˜µì…˜ìœ¼ë¡œ í•„ìš”í•œ 9ê°œ ì»¬ëŸ¼ë§Œ ì½ì–´ì™€ì„œ ParserErrorë¥¼ í•´ê²°í•©ë‹ˆë‹¤.
pop_density_df = pd.read_csv('.\data\ìë©´ë™ ì „ì²´ ì¸êµ¬ë°€ë„.csv',
    names=csv_columns,  # ì»¬ëŸ¼ ì´ë¦„ì„ ê°•ì œë¡œ ì§€ì •
    header=0,           # ì²« ë²ˆì§¸ ì¤„ì„ í—¤ë”ë¡œ ì‚¬ìš© (ì´í›„ namesë¡œ ë®ì–´ì”€)
    usecols=csv_columns # ì§€ì •ëœ ì»¬ëŸ¼ë§Œ ì‚¬ìš©
)

geo_daegu = gpd.read_file(".\data\hangjeongdong_ëŒ€êµ¬ê´‘ì—­ì‹œ.geojson")
geo_gyeongbuk = gpd.read_file(".\data\hangjeongdong_ê²½ìƒë¶ë„.geojson")


# --- 2. ë°ì´í„° ì „ì²˜ë¦¬ ë° ë³‘í•© ---
# 2-1. ì§€ë„ ë°ì´í„°(GeoJSON) ì¤€ë¹„
geo_gunwi = geo_gyeongbuk[geo_gyeongbuk['adm_nm'].str.contains('ê²½ìƒë¶ë„ êµ°ìœ„êµ°', na=False)].copy()
geo_merged = pd.concat([geo_daegu, geo_gunwi], ignore_index=True)
geo_merged = gpd.GeoDataFrame(geo_merged, crs=geo_daegu.crs)
geo_merged['adm_nm'] = geo_merged['adm_nm'].str.strip().str.replace('ê²½ìƒë¶ë„ êµ°ìœ„êµ°', 'êµ°ìœ„êµ°', regex=False)
geo_merged['ìë©´ë™_key'] = geo_merged['adm_nm'].apply(lambda x: x.split(' ')[-1])

# 2-2. ì–‘ìª½ ë°ì´í„°ì˜ í–‰ì •ë™ ì´ë¦„ì—ì„œ ëª¨ë“  íŠ¹ìˆ˜ë¬¸ìì™€ 'ë™'ì„ ì œê±°í•˜ì—¬ í˜•ì‹ì„ í†µì¼í•©ë‹ˆë‹¤.
# (ì˜ˆ: 'ë‘ë¥˜1Â·2ë™' -> 'ë‘ë¥˜12', 'ë‘ë¥˜1ë™2ë™' -> 'ë‘ë¥˜12')
def clean_dong_name(name_series):
    return name_series.str.replace('.', '', regex=False).str.replace('Â·', '', regex=False).str.replace(',', '', regex=False).str.replace('ë™', '', regex=False)

geo_merged['ìë©´ë™_key'] = clean_dong_name(geo_merged['ìë©´ë™_key'])
pop_density_df['ìë©´ë™_key'] = clean_dong_name(pop_density_df['ìë©´ë™'])


# 2-3. í˜•ì‹ì´ í†µì¼ëœ 'ìë©´ë™_key'ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„° ë³‘í•©
merged_gdf = geo_merged.merge(pop_density_df, on='ìë©´ë™_key')


# --- 3. ì§€ë„ ì‹œê°í™” ---
m = folium.Map(location=[35.8714, 128.6014], zoom_start=11)

# [ì—­í•  1] ë°°ê²½ìƒ‰ Choropleth ë ˆì´ì–´
folium.Choropleth(
    geo_data=merged_gdf,
    name='ì¸êµ¬ ë°€ë„',
    data=merged_gdf,
    columns=['ìë©´ë™_key', 'ì¸êµ¬ë°€ë„'],
    key_on='feature.properties.ìë©´ë™_key',
    fill_color='YlOrRd',
    fill_opacity=0.8,
    line_opacity=0,
    legend_name='ì¸êµ¬ ë°€ë„ (ëª…/ã¢)'
).add_to(m)

# [ì—­í•  2] íˆ´íŒ, í…Œë‘ë¦¬ì„ , í•˜ì´ë¼ì´íŠ¸ GeoJson ë ˆì´ì–´
tooltip = folium.features.GeoJsonTooltip(
    fields=['ìë©´ë™', 'ì¸êµ¬ë°€ë„'],  # íˆ´íŒì—ëŠ” ì›ë˜ 'ìë©´ë™' ì´ë¦„ í‘œì‹œ
    aliases=['í–‰ì •ë™:', 'ì¸êµ¬ë°€ë„:'],
    style=("background-color: white; color: #333333; font-family: arial; font-size: 12px; padding: 10px;")
)
highlight_function = lambda x: {'weight': 3, 'color': 'black', 'fillOpacity': 0.5}

folium.GeoJson(
    merged_gdf,
    style_function=lambda x: {'fillColor': 'transparent', 'color': 'black', 'weight': 0.5},
    highlight_function=highlight_function,
    tooltip=tooltip
).add_to(m)

folium.LayerControl().add_to(m)


# --- 4. íŒŒì¼ë¡œ ì €ì¥ ---
# m.save('ëŒ€êµ¬ ì „ì²´ ì¸êµ¬ë°€ë„_map.html')
# print("ğŸ‰ ëª¨ë“  ë¬¸ì œê°€ í•´ê²°ëœ ìµœì¢… ì§€ë„ê°€ 'ëŒ€êµ¬ ì „ì²´ ì¸êµ¬ë°€ë„_map.html' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

```

## (2) ìœ„í—˜ ì·¨ì•½ ì—°ë ¹ ì¸êµ¬ ë°€ë„: â‘  ì–´ë¦°ì´ â‘¡ ê³ ë ¹ì

```{python}

# (2) 4_2_age ############################################################

# ì „ì²˜ë¦¬

# ë™Â·ìÂ·ë©´_ì—°ë ¹ë³„_ì£¼ë¯¼ë“±ë¡ì¸êµ¬_ë‚´êµ­ì¸_ì „ì²´ì—°ë ¹_20250812133656
# https://kosis.daegu.go.kr/statHtml/statHtml.do?orgId=203&tblId=DT_203N100020&lang_mode=ko&vw_cd=MT_OTITLE&list_id=203_B203_05&conn_path=I4

age = pd.read_csv('./data/4_2_age.csv')

age.rename(columns={'í–‰ì •êµ¬ì—­':'í–‰ì •êµ¬',
                    'ë™ìë©´': 'ìë©´ë™'}, 
                    inplace=True)
# age.info()
# age.head()


# ì‰¼í‘œì™€ '-' ì²˜ë¦¬ í›„ ìˆ«ìí˜•ìœ¼ë¡œ ë³€í™˜
for col in ['ê³„', 'ë‚¨ì', 'ì—¬ì']:
    age[col] = (
        age[col]
        .astype(str)               # ë¬¸ìì—´ ë³€í™˜
        .str.replace(',', '', regex=False)  # ì‰¼í‘œ ì œê±°
        .str.strip()               # ì•ë’¤ ê³µë°± ì œê±°
        .replace({'': '0', '-': '0'})       # ë¹ˆê°’ê³¼ '-'ë¥¼ 0ìœ¼ë¡œ
        .astype(int)               # ì •ìˆ˜í˜• ë³€í™˜
    )

# ë³€í™˜ í™•ì¸
print(age.info())
print(age.head())




# ì—°ë ¹ í•„í„°ë§
# age['ì—°ë ¹'].unique()

# ì–´ë¦°ì´: 0~14ì„¸
child = ['0~4ì„¸', '5~9ì„¸', '10~14ì„¸']
age_child = age[age['ì—°ë ¹'].isin(child)]

# ë…¸ì¸: 65ì„¸ ì´ìƒ
senior = ['65~69ì„¸', '70~74ì„¸', '75~79ì„¸','80~84ì„¸', '85~89ì„¸', '90~94ì„¸','95~99ì„¸', '100ì„¸ì´ìƒ']
age_senior = age[age['ì—°ë ¹'].isin(senior)]


# ì–´ë¦°ì´ ì¸êµ¬ í•©ê³„
child_sum = (
    age_child
    .groupby(['í–‰ì •êµ¬', 'ìë©´ë™'], as_index=False)['ê³„']
    .sum()
    .rename(columns={'ê³„': 'ì–´ë¦°ì´ìˆ˜'})
)
# child_sum.info()

# ë…¸ì¸ ì¸êµ¬ í•©ê³„
senior_sum = (
    age_senior
    .groupby(['í–‰ì •êµ¬', 'ìë©´ë™'], as_index=False)['ê³„']
    .sum()
    .rename(columns={'ê³„': 'ê³ ë ¹ììˆ˜'})
)
# senior_sum.info()

```

```{python}

# (1) êµ¬ ì „ì²´ : ['ìë©´ë™'] == 'ì†Œê³„'
child_gu = child_sum[child_sum['ìë©´ë™'] == 'ì†Œê³„'][['í–‰ì •êµ¬','ì–´ë¦°ì´ìˆ˜']]
senior_gu = senior_sum[senior_sum['ìë©´ë™'] == 'ì†Œê³„'][['í–‰ì •êµ¬','ê³ ë ¹ììˆ˜']]

# dens ë°ì´í„°ì™€ ë³‘í•© -> ë©´ì  ì •ë³´
gu_dens_sum_4_age = gu_dens_sum[['í–‰ì •êµ¬','ì¸êµ¬(ëª…)','ë©´ì (ã¢)','ì¸êµ¬ë°€ë„']]

age_gu = pd.concat([gu_dens_sum_4_age.set_index(['í–‰ì •êµ¬']),
                    child_gu.set_index(['í–‰ì •êµ¬']),
                    senior_gu.set_index(['í–‰ì •êµ¬'])],
                    axis=1).reset_index()


# ì—°ë ¹ë³„ ì¸êµ¬ë°€ë„ ê³„ì‚°
age_gu['ì–´ë¦°ì´_ì¸êµ¬ë°€ë„'] = (age_gu['ì–´ë¦°ì´ìˆ˜'] / age_gu['ë©´ì (ã¢)']).round(2)
age_gu['ì–´ë¦°ì´ ì¸êµ¬ë°€ë„_norm'] = scaler.fit_transform(age_gu[['ì–´ë¦°ì´_ì¸êµ¬ë°€ë„']])
age_gu['ê³ ë ¹ì_ì¸êµ¬ë°€ë„'] = (age_gu['ê³ ë ¹ììˆ˜'] / age_gu['ë©´ì (ã¢)']).round(2)
age_gu['ê³ ë ¹ì ì¸êµ¬ë°€ë„_norm'] = scaler.fit_transform(age_gu[['ê³ ë ¹ì_ì¸êµ¬ë°€ë„']])
# age_gu
# len(age_gu['í–‰ì •êµ¬'].unique())


## ì¶œë ¥

age_gu
# age_gu.to_csv("ì–´ë¦°ì´ ê³ ë ¹ì í–‰ì •êµ¬ ì¸êµ¬ë°€ë„.csv", index=False, encoding="utf-8-sig")

```

```{python}
# (2) êµ¬ ë‚´ ë™ìë©´ : ['ìë©´ë™'] != 'ì†Œê³„'
child_dong = child_sum[child_sum['ìë©´ë™'] != 'ì†Œê³„']
senior_dong = senior_sum[senior_sum['ìë©´ë™'] != 'ì†Œê³„']


# dens ë°ì´í„°ì™€ ë³‘í•© -> ë©´ì  ì •ë³´
dong_dens_df = pd.concat(gu_dfs.values(), ignore_index=True)
dong_dens_4_age = dong_dens_df[['í–‰ì •êµ¬','ìë©´ë™','ì¸êµ¬(ëª…)','ë©´ì (ã¢)','ì¸êµ¬ë°€ë„']]
# dong_dens_4_age['ìë©´ë™'].unique()

age_dong = pd.concat([dong_dens_4_age.set_index(['í–‰ì •êµ¬','ìë©´ë™']),
                    child_dong.set_index(['í–‰ì •êµ¬','ìë©´ë™']),
                    senior_dong.set_index(['í–‰ì •êµ¬','ìë©´ë™'])],
                    axis=1).reset_index()


# ì—°ë ¹ë³„ ì¸êµ¬ë°€ë„ ê³„ì‚°
age_dong['ì–´ë¦°ì´_ì¸êµ¬ë°€ë„'] = (age_dong['ì–´ë¦°ì´ìˆ˜'] / age_dong['ë©´ì (ã¢)']).round(2)
age_dong['ì–´ë¦°ì´ ì¸êµ¬ë°€ë„_norm'] = scaler.fit_transform(age_dong[['ì–´ë¦°ì´_ì¸êµ¬ë°€ë„']])
age_dong['ê³ ë ¹ì_ì¸êµ¬ë°€ë„'] = (age_dong['ê³ ë ¹ììˆ˜'] / age_dong['ë©´ì (ã¢)']).round(2)
age_dong['ê³ ë ¹ì ì¸êµ¬ë°€ë„_norm'] = scaler.fit_transform(age_dong[['ê³ ë ¹ì_ì¸êµ¬ë°€ë„']])

# age_dong
# len(age_dong['í–‰ì •êµ¬'].unique())
# len(age_dong['ìë©´ë™'].unique())


## ì¶œë ¥
# age_dong.to_csv("ì–´ë¦°ì´ ê³ ë ¹ì ìë©´ë™ ì¸êµ¬ë°€ë„.csv", index=False, encoding="utf-8-sig")

# Jung_gu = 
age_dong[age_dong['í–‰ì •êµ¬'] == 'ì¤‘êµ¬']
# Dong_gu = 
age_dong[age_dong['í–‰ì •êµ¬'] == 'ë™êµ¬']
# Seo_gu = 
age_dong[age_dong['í–‰ì •êµ¬'] == 'ì„œêµ¬']
# Nam_gu = 
age_dong[age_dong['í–‰ì •êµ¬'] == 'ë‚¨êµ¬']
# Buk_gu = 
age_dong[age_dong['í–‰ì •êµ¬'] == 'ë¶êµ¬']
# Suseong_gu = 
age_dong[age_dong['í–‰ì •êµ¬'] == 'ìˆ˜ì„±êµ¬']
# Dalseo_gu = 
age_dong[age_dong['í–‰ì •êµ¬'] == 'ë‹¬ì„œêµ¬']
# Dalseong_gun = 
age_dong[age_dong['í–‰ì •êµ¬'] == 'ë‹¬ì„±êµ°']
# Gunwi_gun = 
age_dong[age_dong['í–‰ì •êµ¬'] == 'êµ°ìœ„êµ°']

```

```{python}
# (3) sort

## êµ¬
# age_gu.columns
age_gu_child = age_gu[['í–‰ì •êµ¬', 'ì¸êµ¬(ëª…)', 'ë©´ì (ã¢)', 'ì¸êµ¬ë°€ë„', 
                       'ì–´ë¦°ì´ìˆ˜', 'ì–´ë¦°ì´_ì¸êµ¬ë°€ë„', 'ì–´ë¦°ì´ ì¸êµ¬ë°€ë„_norm']]
age_gu_senior = age_gu[['í–‰ì •êµ¬', 'ì¸êµ¬(ëª…)', 'ë©´ì (ã¢)', 'ì¸êµ¬ë°€ë„', 
                        'ê³ ë ¹ììˆ˜', 'ê³ ë ¹ì_ì¸êµ¬ë°€ë„', 'ê³ ë ¹ì ì¸êµ¬ë°€ë„_norm']]

## ì¶œë ¥

# í–‰ì •êµ¬ ì–´ë¦°ì´ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ
age_gu_child.sort_values(['ì–´ë¦°ì´ìˆ˜'], ascending=False)
# í–‰ì •êµ¬ ì–´ë¦°ì´ ì¸êµ¬ë°€ë„ ë‚´ë¦¼ì°¨ìˆœ
age_gu_child.sort_values(['ì–´ë¦°ì´_ì¸êµ¬ë°€ë„'], ascending=False)


# í–‰ì •êµ¬ ê³ ë ¹ììˆ˜ ë‚´ë¦¼ì°¨ìˆœ
age_gu_senior.sort_values(['ê³ ë ¹ììˆ˜'], ascending=False)
# í–‰ì •êµ¬ ê³ ë ¹ì ì¸êµ¬ë°€ë„ ë‚´ë¦¼ì°¨ìˆœ
age_gu_senior.sort_values(['ê³ ë ¹ì_ì¸êµ¬ë°€ë„'], ascending=False)
```

```{python}
## ë™
age_dong_child = age_dong[['í–‰ì •êµ¬', 'ìë©´ë™', 'ì¸êµ¬(ëª…)', 'ë©´ì (ã¢)', 'ì¸êµ¬ë°€ë„', 
                           'ì–´ë¦°ì´ìˆ˜', 'ì–´ë¦°ì´_ì¸êµ¬ë°€ë„', 'ì–´ë¦°ì´ ì¸êµ¬ë°€ë„_norm']]
age_dong_senior = age_dong[['í–‰ì •êµ¬', 'ìë©´ë™', 'ì¸êµ¬(ëª…)', 'ë©´ì (ã¢)', 'ì¸êµ¬ë°€ë„',
                            'ê³ ë ¹ììˆ˜', 'ê³ ë ¹ì_ì¸êµ¬ë°€ë„', 'ê³ ë ¹ì ì¸êµ¬ë°€ë„_norm']]

## ëŒ€êµ¬ì‹œ ì „ì²´ ë™ ë‚´ sort 10

age_dong_child.sort_values(['ì–´ë¦°ì´ìˆ˜'], ascending=False).head(10)
age_dong_child.sort_values(['ì–´ë¦°ì´_ì¸êµ¬ë°€ë„'], ascending=False).head(10)
age_dong_senior.sort_values(['ê³ ë ¹ììˆ˜'], ascending=False).head(10)
age_dong_senior.sort_values(['ê³ ë ¹ì_ì¸êµ¬ë°€ë„'], ascending=False).head(10)


## ê° í–‰ì •êµ¬ ë‚´ sort
# age_dong.info()
# len(age_dong['í–‰ì •êµ¬'].unique())



## ì¶œë ¥

# ì¤‘êµ¬: Jung_gu = 
age_dong_child[age_dong_child['í–‰ì •êµ¬'] == 'ì¤‘êµ¬'].sort_values(['ì–´ë¦°ì´ìˆ˜'], ascending=False)
age_dong_senior[age_dong_senior['í–‰ì •êµ¬'] == 'ì¤‘êµ¬'].sort_values(['ê³ ë ¹ììˆ˜'], ascending=False)

# ë™êµ¬: Dong_gu = 
age_dong_child[age_dong_child['í–‰ì •êµ¬'] == 'ë™êµ¬'].sort_values(['ì–´ë¦°ì´ìˆ˜'], ascending=False)
age_dong_senior[age_dong_senior['í–‰ì •êµ¬'] == 'ë™êµ¬'].sort_values(['ê³ ë ¹ììˆ˜'], ascending=False)

# ì„œêµ¬: Seo_gu = 
age_dong_child[age_dong_child['í–‰ì •êµ¬'] == 'ì„œêµ¬'].sort_values(['ì–´ë¦°ì´ìˆ˜'], ascending=False)
age_dong_senior[age_dong_senior['í–‰ì •êµ¬'] == 'ì„œêµ¬'].sort_values(['ê³ ë ¹ììˆ˜'], ascending=False)

# ë‚¨êµ¬: Nam_gu = 
age_dong_child[age_dong_child['í–‰ì •êµ¬'] == 'ë‚¨êµ¬'].sort_values(['ì–´ë¦°ì´ìˆ˜'], ascending=False)
age_dong_senior[age_dong_senior['í–‰ì •êµ¬'] == 'ë‚¨êµ¬'].sort_values(['ê³ ë ¹ììˆ˜'], ascending=False)

# ë¶êµ¬: Buk_gu = 
age_dong_child[age_dong_child['í–‰ì •êµ¬'] == 'ë¶êµ¬'].sort_values(['ì–´ë¦°ì´ìˆ˜'], ascending=False)
age_dong_senior[age_dong_senior['í–‰ì •êµ¬'] == 'ë¶êµ¬'].sort_values(['ê³ ë ¹ììˆ˜'], ascending=False)

# ìˆ˜ì„±êµ¬: Suseong_gu = 
age_dong_child[age_dong_child['í–‰ì •êµ¬'] == 'ìˆ˜ì„±êµ¬'].sort_values(['ì–´ë¦°ì´ìˆ˜'], ascending=False)
age_dong_senior[age_dong_senior['í–‰ì •êµ¬'] == 'ìˆ˜ì„±êµ¬'].sort_values(['ê³ ë ¹ììˆ˜'], ascending=False)

# ë‹¬ì„œêµ¬: Dalseo_gu = 
age_dong_child[age_dong_child['í–‰ì •êµ¬'] == 'ë‹¬ì„œêµ¬'].sort_values(['ì–´ë¦°ì´ìˆ˜'], ascending=False)
age_dong_senior[age_dong_senior['í–‰ì •êµ¬'] == 'ë‹¬ì„œêµ¬'].sort_values(['ê³ ë ¹ììˆ˜'], ascending=False)

# ë‹¬ì„±êµ°: Dalseong_gun = 
age_dong_child[age_dong_child['í–‰ì •êµ¬'] == 'ë‹¬ì„±êµ°'].sort_values(['ì–´ë¦°ì´ìˆ˜'], ascending=False)
age_dong_senior[age_dong_senior['í–‰ì •êµ¬'] == 'ë‹¬ì„±êµ°'].sort_values(['ê³ ë ¹ììˆ˜'], ascending=False)

# êµ°ìœ„êµ°: Gunwi_gun = 
age_dong_child[age_dong_child['í–‰ì •êµ¬'] == 'êµ°ìœ„êµ°'].sort_values(['ì–´ë¦°ì´ìˆ˜'], ascending=False)
age_dong_senior[age_dong_senior['í–‰ì •êµ¬'] == 'êµ°ìœ„êµ°'].sort_values(['ê³ ë ¹ììˆ˜'], ascending=False)

```

```{python}
# ì§€ë„ ì‹œê°í™”

# ì–´ë¦°ì´ ì¸êµ¬ë°€ë„


# --- 1. ë°ì´í„° ë¡œë“œ ---
# 'ì–´ë¦°ì´ ë°€ë„.csv' íŒŒì¼ì„ ì½ì–´ì˜µë‹ˆë‹¤. encoding='cp949'ë¥¼ ì¶”ê°€í•˜ì—¬ í•œê¸€ ê¹¨ì§ì„ ë°©ì§€í•©ë‹ˆë‹¤.
pop_density_df = pd.read_csv('.\data\ì–´ë¦°ì´ ê³ ë ¹ì ìë©´ë™ ì¸êµ¬ë°€ë„.csv') # <--- 1. íŒŒì¼ ì´ë¦„ ìˆ˜ì •


geo_daegu = gpd.read_file(".\data\hangjeongdong_ëŒ€êµ¬ê´‘ì—­ì‹œ.geojson")
geo_gyeongbuk = gpd.read_file(".\data\hangjeongdong_ê²½ìƒë¶ë„.geojson")


# --- 2. ë°ì´í„° ì „ì²˜ë¦¬ ë° ë³‘í•© ---
# 2-1. ì§€ë„ ë°ì´í„°(GeoJSON) ì¤€ë¹„ (ê¸°ì¡´ê³¼ ë™ì¼)
geo_gunwi = geo_gyeongbuk[geo_gyeongbuk['adm_nm'].str.contains('ê²½ìƒë¶ë„ êµ°ìœ„êµ°', na=False)].copy()
geo_merged = pd.concat([geo_daegu, geo_gunwi], ignore_index=True)
geo_merged = gpd.GeoDataFrame(geo_merged, crs=geo_daegu.crs)
geo_merged['adm_nm'] = geo_merged['adm_nm'].str.strip().str.replace('ê²½ìƒë¶ë„ êµ°ìœ„êµ°', 'êµ°ìœ„êµ°', regex=False)
geo_merged['ìë©´ë™_key'] = geo_merged['adm_nm'].apply(lambda x: x.split(' ')[-1])

# 2-2. ì–‘ìª½ ë°ì´í„°ì˜ í–‰ì •ë™ ì´ë¦„ì—ì„œ ëª¨ë“  íŠ¹ìˆ˜ë¬¸ìì™€ 'ë™'ì„ ì œê±°í•˜ì—¬ í˜•ì‹ì„ í†µì¼ (ê¸°ì¡´ê³¼ ë™ì¼)
def clean_dong_name(name_series):
    return name_series.str.replace('.', '', regex=False).str.replace('Â·', '', regex=False).str.replace(',', '', regex=False).str.replace('ë™', '', regex=False)

geo_merged['ìë©´ë™_key'] = clean_dong_name(geo_merged['ìë©´ë™_key'])
pop_density_df['ìë©´ë™_key'] = clean_dong_name(pop_density_df['ìë©´ë™'])


# 2-3. í˜•ì‹ì´ í†µì¼ëœ 'ìë©´ë™_key'ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„° ë³‘í•©
merged_gdf = geo_merged.merge(pop_density_df, on='ìë©´ë™_key')


# --- 3. ì§€ë„ ì‹œê°í™” ---
m = folium.Map(location=[35.8714, 128.6014], zoom_start=11)

# [ì—­í•  1] ë°°ê²½ìƒ‰ Choropleth ë ˆì´ì–´
folium.Choropleth(
    geo_data=merged_gdf,
    name='ì–´ë¦°ì´ ì¸êµ¬ ë°€ë„', # <--- 2. ì´ë¦„ ìˆ˜ì •
    data=merged_gdf,
    columns=['ìë©´ë™_key', 'ì–´ë¦°ì´_ì¸êµ¬ë°€ë„'], # <--- 3. ì»¬ëŸ¼ ìˆ˜ì •
    key_on='feature.properties.ìë©´ë™_key',
    fill_color='YlOrRd',
    fill_opacity=0.8,
    line_opacity=0,
    legend_name='ì–´ë¦°ì´ ì¸êµ¬ ë°€ë„ (ëª…/ã¢)' # <--- 4. ë²”ë¡€ ì´ë¦„ ìˆ˜ì •
).add_to(m)

# [ì—­í•  2] íˆ´íŒ, í…Œë‘ë¦¬ì„ , í•˜ì´ë¼ì´íŠ¸ GeoJson ë ˆì´ì–´
tooltip = folium.features.GeoJsonTooltip(
    fields=['ìë©´ë™', 'ì–´ë¦°ì´_ì¸êµ¬ë°€ë„'],  # <--- 5. íˆ´íŒ ì»¬ëŸ¼ ìˆ˜ì •
    aliases=['í–‰ì •ë™:', 'ì–´ë¦°ì´ ì¸êµ¬ë°€ë„:'], # <--- 6. íˆ´íŒ ë³„ëª… ìˆ˜ì •
    style=("background-color: white; color: #333333; font-family: arial; font-size: 12px; padding: 10px;")
)
highlight_function = lambda x: {'weight': 3, 'color': 'black', 'fillOpacity': 0.5}

folium.GeoJson(
    merged_gdf,
    style_function=lambda x: {'fillColor': 'transparent', 'color': 'black', 'weight': 0.5},
    highlight_function=highlight_function,
    tooltip=tooltip
).add_to(m)

folium.LayerControl().add_to(m)


# --- 4. íŒŒì¼ë¡œ ì €ì¥ ---
# m.save('ëŒ€êµ¬ ì–´ë¦°ì´ ì¸êµ¬ë°€ë„_map.html') # <--- 7. ì €ì¥ íŒŒì¼ ì´ë¦„ ìˆ˜ì •
# print("ğŸ‰ ì–´ë¦°ì´ ì¸êµ¬ ë°€ë„ ì§€ë„ê°€ ëŒ€êµ¬ ì–´ë¦°ì´ ì¸êµ¬ë°€ë„_map.html' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

```

```{python}
# ì§€ë„ ì‹œê°í™”

# ê³ ë ¹ì ì¸êµ¬ë°€ë„


# --- 1. ë°ì´í„° ë¡œë“œ ---
# 'ê³ ë ¹ì ì¸êµ¬ë°€ë„.csv' íŒŒì¼ì„ ì½ì–´ì˜µë‹ˆë‹¤. encoding='cp949'ë¥¼ ì¶”ê°€í•˜ì—¬ í•œê¸€ ê¹¨ì§ì„ ë°©ì§€í•©ë‹ˆë‹¤.
pop_density_df = pd.read_csv('.\data\ì–´ë¦°ì´ ê³ ë ¹ì ìë©´ë™ ì¸êµ¬ë°€ë„.csv')

geo_daegu = gpd.read_file(".\data\hangjeongdong_ëŒ€êµ¬ê´‘ì—­ì‹œ.geojson")
geo_gyeongbuk = gpd.read_file(".\data\hangjeongdong_ê²½ìƒë¶ë„.geojson")


# --- 2. ë°ì´í„° ì „ì²˜ë¦¬ ë° ë³‘í•© ---
# 2-1. ì§€ë„ ë°ì´í„°(GeoJSON) ì¤€ë¹„ (ê¸°ì¡´ê³¼ ë™ì¼)
geo_gunwi = geo_gyeongbuk[geo_gyeongbuk['adm_nm'].str.contains('ê²½ìƒë¶ë„ êµ°ìœ„êµ°', na=False)].copy()
geo_merged = pd.concat([geo_daegu, geo_gunwi], ignore_index=True)
geo_merged = gpd.GeoDataFrame(geo_merged, crs=geo_daegu.crs)
geo_merged['adm_nm'] = geo_merged['adm_nm'].str.strip().str.replace('ê²½ìƒë¶ë„ êµ°ìœ„êµ°', 'êµ°ìœ„êµ°', regex=False)
geo_merged['ìë©´ë™_key'] = geo_merged['adm_nm'].apply(lambda x: x.split(' ')[-1])

# 2-2. ì–‘ìª½ ë°ì´í„°ì˜ í–‰ì •ë™ ì´ë¦„ì—ì„œ ëª¨ë“  íŠ¹ìˆ˜ë¬¸ìì™€ 'ë™'ì„ ì œê±°í•˜ì—¬ í˜•ì‹ì„ í†µì¼ (ê¸°ì¡´ê³¼ ë™ì¼)
def clean_dong_name(name_series):
    return name_series.str.replace('.', '', regex=False).str.replace('Â·', '', regex=False).str.replace(',', '', regex=False).str.replace('ë™', '', regex=False)

geo_merged['ìë©´ë™_key'] = clean_dong_name(geo_merged['ìë©´ë™_key'])
pop_density_df['ìë©´ë™_key'] = clean_dong_name(pop_density_df['ìë©´ë™'])


# 2-3. í˜•ì‹ì´ í†µì¼ëœ 'ìë©´ë™_key'ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„° ë³‘í•©
merged_gdf = geo_merged.merge(pop_density_df, on='ìë©´ë™_key')


# --- 3. ì§€ë„ ì‹œê°í™” ---
m = folium.Map(location=[35.8714, 128.6014], zoom_start=11)

# [ì—­í•  1] ë°°ê²½ìƒ‰ Choropleth ë ˆì´ì–´
folium.Choropleth(
    geo_data=merged_gdf,
    name='ê³ ë ¹ì ì¸êµ¬ ë°€ë„', # <--- 2. ì´ë¦„ ìˆ˜ì •
    data=merged_gdf,
    columns=['ìë©´ë™_key', 'ê³ ë ¹ì_ì¸êµ¬ë°€ë„'], # <--- 3. ì»¬ëŸ¼ ìˆ˜ì •
    key_on='feature.properties.ìë©´ë™_key',
    fill_color='YlOrRd',
    fill_opacity=0.8,
    line_opacity=0,
    legend_name='ê³ ë ¹ì ì¸êµ¬ ë°€ë„ (ëª…/ã¢)' # <--- 4. ë²”ë¡€ ì´ë¦„ ìˆ˜ì •
).add_to(m)

# [ì—­í•  2] íˆ´íŒ, í…Œë‘ë¦¬ì„ , í•˜ì´ë¼ì´íŠ¸ GeoJson ë ˆì´ì–´
tooltip = folium.features.GeoJsonTooltip(
    fields=['ìë©´ë™', 'ê³ ë ¹ì_ì¸êµ¬ë°€ë„'],  # <--- 5. íˆ´íŒ ì»¬ëŸ¼ ìˆ˜ì •
    aliases=['í–‰ì •ë™:', 'ê³ ë ¹ì ì¸êµ¬ë°€ë„:'], # <--- 6. íˆ´íŒ ë³„ëª… ìˆ˜ì •
    style=("background-color: white; color: #333333; font-family: arial; font-size: 12px; padding: 10px;")
)
highlight_function = lambda x: {'weight': 3, 'color': 'black', 'fillOpacity': 0.5}

folium.GeoJson(
    merged_gdf,
    style_function=lambda x: {'fillColor': 'transparent', 'color': 'black', 'weight': 0.5},
    highlight_function=highlight_function,
    tooltip=tooltip
).add_to(m)

folium.LayerControl().add_to(m)


# --- 4. íŒŒì¼ë¡œ ì €ì¥ ---
# m.save('ëŒ€êµ¬ ê³ ë ¹ì ì¸êµ¬ë°€ë„_map.html') # <--- 7. ì €ì¥ íŒŒì¼ ì´ë¦„ ìˆ˜ì •
# print("ğŸ‰ ê³ ë ¹ì ì¸êµ¬ ë°€ë„ ì§€ë„ê°€ 'ëŒ€êµ¬ ê³ ë ¹ì ì¸êµ¬ë°€ë„_map.html' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

```

## (3) ì™¸êµ­ì¸ ì¸êµ¬ ë°€ë„

```{python}
# (3) 4_3_foreign ############################################################
# ë™Â·ìÂ·ë©´ë³„_ë‚´Â·ì™¸êµ­ì¸_20250812141415
# https://kosis.daegu.go.kr/statHtml/statHtml.do?orgId=203&tblId=DT_203N100015&lang_mode=ko&vw_cd=MT_OTITLE&list_id=203_B203_05&conn_path=I4

# ì „ì²˜ë¦¬

fore = pd.read_csv('./data/4_3_foreign.csv')

for col in ['ì´ì¸êµ¬', 'í•œêµ­ì¸', 'ì™¸êµ­ì¸']:
    fore[col] = (
        fore[col]
        .astype(str)
        .str.replace(',', '')
        .str.strip()
        .replace('-', '0')
        .astype(int)
    )

fore.rename(columns={'í–‰ì •êµ¬ì—­':'í–‰ì •êµ¬',
                    'ë™ìë©´': 'ìë©´ë™',
                    'ì´ì¸êµ¬': 'ì´ ì¸êµ¬(ëª…)',
                    'í•œêµ­ì¸': 'í•œêµ­ì¸ ì¸êµ¬(ëª…)', 
                    'ì™¸êµ­ì¸': 'ì™¸êµ­ì¸ ì¸êµ¬(ëª…)'}, 
                    inplace=True)

fore = fore[(fore['í–‰ì •êµ¬'] != 'í•©ê³„') & (fore['ì„±ë³„'] == 'ê³„')]

fore.info()
fore.head()


# í–‰ì •êµ¬ë³„ ì™¸êµ­ì¸ ì¸êµ¬
fore_gu = fore[fore['ìë©´ë™'] == 'ì†Œê³„'][['í–‰ì •êµ¬', 'ì´ ì¸êµ¬(ëª…)', 'ì™¸êµ­ì¸ ì¸êµ¬(ëª…)']]

# ìë©´ë™ë³„ ì™¸êµ­ì¸ ì¸êµ¬
fore_dong = fore[fore['ìë©´ë™'] != 'ì†Œê³„'][['í–‰ì •êµ¬', 'ìë©´ë™', 'ì´ ì¸êµ¬(ëª…)', 'ì™¸êµ­ì¸ ì¸êµ¬(ëª…)']]


```

```{python}

# ì™¸êµ­ì¸ ì¸êµ¬ ë°€ë„

# (1) í–‰ì •êµ¬ë³„ ì™¸êµ­ì¸ ì¸êµ¬ ë°€ë„
gu_area = dens.groupby('í–‰ì •êµ¬')[['ë©´ì (ã¢)']].sum().reset_index()

fore_gu = pd.merge(
    fore_gu,
    gu_area,
    on='í–‰ì •êµ¬',
    how='left'
)

fore_gu['ì™¸êµ­ì¸ ì¸êµ¬ë°€ë„'] = (fore_gu['ì™¸êµ­ì¸ ì¸êµ¬(ëª…)'] / fore_gu['ë©´ì (ã¢)']).round(2)
# fore_gu

# (2) ìë©´ë™ë³„ ì™¸êµ­ì¸ ì¸êµ¬ ë°€ë„
dong_area = dens.groupby('ìë©´ë™')[['ë©´ì (ã¢)']].sum().reset_index()

fore_dong = pd.merge(
    fore_dong,
    dong_area,
    on='ìë©´ë™',
    how='left'
)

fore_dong['ì™¸êµ­ì¸ ì¸êµ¬ë°€ë„'] = (fore_dong['ì™¸êµ­ì¸ ì¸êµ¬(ëª…)'] / fore_dong['ë©´ì (ã¢)']).round(2)
# fore_dong


## ì¶œë ¥

# (1) í–‰ì •êµ¬ë³„ ì™¸êµ­ì¸ ì¸êµ¬ ë°€ë„
# fore_gu

fore_gu.sort_values(by='ì™¸êµ­ì¸ ì¸êµ¬ë°€ë„', ascending=False)


# (2) ìë©´ë™ë³„ ì™¸êµ­ì¸ ì¸êµ¬ ë°€ë„
# fore_dong

# ìë©´ë™ë³„ ì™¸êµ­ì¸ ì¸êµ¬ ë°€ë„ top 10
fore_gu.sort_values(by='ì™¸êµ­ì¸ ì¸êµ¬ë°€ë„', ascending=False).head(10)


```

```{python}

# ì™¸êµ­ì¸ ì¸êµ¬ë°€ë„ ì •ê·œí™”

# (1) í–‰ì •êµ¬ë³„ ì™¸êµ­ì¸ ì¸êµ¬ ë°€ë„
fore_gu['ì™¸êµ­ì¸ ì¸êµ¬ë°€ë„_norm'] = scaler.fit_transform(
    fore_gu[['ì™¸êµ­ì¸ ì¸êµ¬ë°€ë„']])



# (2) ìë©´ë™ë³„ ì™¸êµ­ì¸ ì¸êµ¬ ë°€ë„
fore_dong['ì™¸êµ­ì¸ ì¸êµ¬ë°€ë„_norm'] = scaler.fit_transform(
    fore_dong[['ì™¸êµ­ì¸ ì¸êµ¬ë°€ë„']])

# í–‰ì •êµ¬ë³„ë¡œ ë°ì´í„°í”„ë ˆì„ ë¶„ë¦¬
# Jung_gu      = 
fore_dong[fore_dong['í–‰ì •êµ¬'] == 'ì¤‘êµ¬']
# Dong_gu      = 
fore_dong[fore_dong['í–‰ì •êµ¬'] == 'ë™êµ¬']
# Seo_gu       = 
fore_dong[fore_dong['í–‰ì •êµ¬'] == 'ì„œêµ¬']
# Nam_gu       = 
fore_dong[fore_dong['í–‰ì •êµ¬'] == 'ë‚¨êµ¬']
# Buk_gu       = 
fore_dong[fore_dong['í–‰ì •êµ¬'] == 'ë¶êµ¬']
# Suseong_gu   = 
fore_dong[fore_dong['í–‰ì •êµ¬'] == 'ìˆ˜ì„±êµ¬']
# Dalseo_gu    = 
fore_dong[fore_dong['í–‰ì •êµ¬'] == 'ë‹¬ì„œêµ¬']
# Dalseong_gun = 
fore_dong[fore_dong['í–‰ì •êµ¬'] == 'ë‹¬ì„±êµ°']
# Gunwi_gun    = 
fore_dong[fore_dong['í–‰ì •êµ¬'] == 'êµ°ìœ„êµ°']



# ì¶œë ¥

# fore_gu.to_csv("í–‰ì •êµ¬ ì™¸êµ­ì¸ ì¸êµ¬.csv", index=False, encoding="utf-8-sig")
# fore_dong.to_csv("ìë©´ë™ ì™¸êµ­ì¸ ì¸êµ¬.csv", index=False, encoding="utf-8-sig")


```

```{python}
# ì§€ë„ ì‹œê°í™”

# --- 1. ë°ì´í„° ë¡œë“œ ---
# 'ìë©´ë™ ì™¸êµ­ì¸ ì¸êµ¬.csv' íŒŒì¼ì„ ì½ì–´ì˜µë‹ˆë‹¤. encoding='cp949'ë¥¼ ì¶”ê°€í•˜ì—¬ í•œê¸€ ê¹¨ì§ì„ ë°©ì§€í•©ë‹ˆë‹¤.
pop_density_df = pd.read_csv('.\data\ìë©´ë™ ì™¸êµ­ì¸ ì¸êµ¬.csv')

geo_daegu = gpd.read_file(".\data\hangjeongdong_ëŒ€êµ¬ê´‘ì—­ì‹œ.geojson")
geo_gyeongbuk = gpd.read_file(".\data\hangjeongdong_ê²½ìƒë¶ë„.geojson")


# --- 2. ë°ì´í„° ì „ì²˜ë¦¬ ë° ë³‘í•© ---
# 2-1. ì§€ë„ ë°ì´í„°(GeoJSON) ì¤€ë¹„ (ê¸°ì¡´ê³¼ ë™ì¼)
geo_gunwi = geo_gyeongbuk[geo_gyeongbuk['adm_nm'].str.contains('ê²½ìƒë¶ë„ êµ°ìœ„êµ°', na=False)].copy()
geo_merged = pd.concat([geo_daegu, geo_gunwi], ignore_index=True)
geo_merged = gpd.GeoDataFrame(geo_merged, crs=geo_daegu.crs)
geo_merged['adm_nm'] = geo_merged['adm_nm'].str.strip().str.replace('ê²½ìƒë¶ë„ êµ°ìœ„êµ°', 'êµ°ìœ„êµ°', regex=False)
geo_merged['ìë©´ë™_key'] = geo_merged['adm_nm'].apply(lambda x: x.split(' ')[-1])

# 2-2. ì–‘ìª½ ë°ì´í„°ì˜ í–‰ì •ë™ ì´ë¦„ì—ì„œ ëª¨ë“  íŠ¹ìˆ˜ë¬¸ìì™€ 'ë™'ì„ ì œê±°í•˜ì—¬ í˜•ì‹ì„ í†µì¼ (ê¸°ì¡´ê³¼ ë™ì¼)
def clean_dong_name(name_series):
    return name_series.str.replace('.', '', regex=False).str.replace('Â·', '', regex=False).str.replace(',', '', regex=False).str.replace('ë™', '', regex=False)

geo_merged['ìë©´ë™_key'] = clean_dong_name(geo_merged['ìë©´ë™_key'])
pop_density_df['ìë©´ë™_key'] = clean_dong_name(pop_density_df['ìë©´ë™'])


# 2-3. í˜•ì‹ì´ í†µì¼ëœ 'ìë©´ë™_key'ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„° ë³‘í•©
merged_gdf = geo_merged.merge(pop_density_df, on='ìë©´ë™_key')


# --- 3. ì§€ë„ ì‹œê°í™” ---
m = folium.Map(location=[35.8714, 128.6014], zoom_start=11)

# [ì—­í•  1] ë°°ê²½ìƒ‰ Choropleth ë ˆì´ì–´
folium.Choropleth(
    geo_data=merged_gdf,
    name='ì™¸êµ­ì¸ ì¸êµ¬ ë°€ë„',
    data=merged_gdf,
    columns=['ìë©´ë™_key', 'ì™¸êµ­ì¸ ì¸êµ¬ë°€ë„'],
    key_on='feature.properties.ìë©´ë™_key',
    fill_color='YlOrRd',  # <--- ìƒ‰ìƒ ìŠ¤ì¼€ì¼ ì›ë³µ
    fill_opacity=0.8,
    line_opacity=0,
    legend_name='ì™¸êµ­ì¸ ì¸êµ¬ ë°€ë„ (ëª…/ã¢)'
).add_to(m)

# [ì—­í•  2] íˆ´íŒ, í…Œë‘ë¦¬ì„ , í•˜ì´ë¼ì´íŠ¸ GeoJson ë ˆì´ì–´
tooltip = folium.features.GeoJsonTooltip(
    fields=['ìë©´ë™', 'ì™¸êµ­ì¸ ì¸êµ¬ë°€ë„'],
    aliases=['í–‰ì •ë™:', 'ì™¸êµ­ì¸ ì¸êµ¬ë°€ë„:'],
    style=("background-color: white; color: #333333; font-family: arial; font-size: 12px; padding: 10px;")
)
highlight_function = lambda x: {'weight': 3, 'color': 'black', 'fillOpacity': 0.5}

folium.GeoJson(
    merged_gdf,
    style_function=lambda x: {'fillColor': 'transparent', 'color': 'black', 'weight': 0.5},
    highlight_function=highlight_function,
    tooltip=tooltip
).add_to(m)

folium.LayerControl().add_to(m)


# --- 4. íŒŒì¼ë¡œ ì €ì¥ ---
# m.save('ëŒ€êµ¬ ì™¸êµ­ì¸ ì¸êµ¬ë°€ë„_map.html')
# print("ğŸ‰ ì™¸êµ­ì¸ ì¸êµ¬ ë°€ë„ ì§€ë„ê°€ 'ëŒ€êµ¬ ì™¸êµ­ì¸ ì¸êµ¬ë°€ë„_map.html' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

```

## () ìš°ì„ ëŒ€ì‘ í•„ìš”: ì¸êµ¬ë°€ë„ ìƒìœ„ 20% + ê³ ë ¹ììˆ˜ ìƒìœ„ 30%

```{python}
# dens.info()
# age.info()
# fore.info()


# 1) ë°ì´í„° ì „ì²˜ë¦¬
age['ê³„'] = pd.to_numeric(age['ê³„'], errors='coerce')
age['ì—°ë ¹'] = age['ì—°ë ¹'].astype(str)

fore['ì´ ì¸êµ¬(ëª…)'] = pd.to_numeric(fore['ì´ ì¸êµ¬(ëª…)'], errors='coerce')
fore['ì™¸êµ­ì¸ ì¸êµ¬(ëª…)'] = pd.to_numeric(fore['ì™¸êµ­ì¸ ì¸êµ¬(ëª…)'], errors='coerce')

# 2) ê³ ë ¹ì(65ì„¸ ì´ìƒ) ì¶”ì¶œ ë° ìë©´ë™ë³„ í•©ì‚°
# age_65 = age[age['ì—°ë ¹'].str.contains(r'^(65|70|75|80|85|90|95|100)', regex=True)]
age_65 = age[age['ì—°ë ¹'].str.contains(r'^65|^70|^75|^80|^85|^90|^95|^100')]

age_65_sum = (
    age_65.groupby(['í–‰ì •êµ¬', 'ìë©´ë™'])['ê³„']
    .sum()
    .reset_index()
    .rename(columns={'ê³„': 'ê³ ë ¹ììˆ˜'})
)

# 3) ì™¸êµ­ì¸ ë¹„ìœ¨ ê³„ì‚° (ìë©´ë™ë³„)
fore_sum = (
    fore.groupby(['í–‰ì •êµ¬', 'ìë©´ë™'], as_index=False)[['ì´ ì¸êµ¬(ëª…)', 'ì™¸êµ­ì¸ ì¸êµ¬(ëª…)']].sum()
)
fore_sum['ì™¸êµ­ì¸ë¹„ìœ¨'] = fore_sum['ì™¸êµ­ì¸ ì¸êµ¬(ëª…)'] / fore_sum['ì´ ì¸êµ¬(ëª…)']

# 4) densì™€ ë³‘í•© (ìë©´ë™ ë‹¨ìœ„)
# í•„ìš”ì‹œ 'ì†Œê³„' ë“± í•©ê³„ í–‰ ì œê±°
dens_clean = dens[~dens['ìë©´ë™'].str.contains('ì†Œê³„|í•©ê³„')]

df_merge = dens_clean.merge(age_65_sum, on=['í–‰ì •êµ¬', 'ìë©´ë™'], how='left')
df_merge = df_merge.merge(fore_sum[['í–‰ì •êµ¬', 'ìë©´ë™', 'ì™¸êµ­ì¸ë¹„ìœ¨']], on=['í–‰ì •êµ¬', 'ìë©´ë™'], how='left')

# 5) ê²°ì¸¡ì¹˜ ì²˜ë¦¬
df_merge['ê³ ë ¹ììˆ˜'] = df_merge['ê³ ë ¹ììˆ˜'].fillna(0).astype(int)
df_merge['ì™¸êµ­ì¸ë¹„ìœ¨'] = df_merge['ì™¸êµ­ì¸ë¹„ìœ¨'].fillna(0)

# 6) ìš°ì„  ëŒ€ì‘ ì§€ì—­ ì¡°ê±´
pop_density_thresh = df_merge['ì¸êµ¬ë°€ë„'].quantile(0.8)
elderly_thresh = df_merge['ê³ ë ¹ììˆ˜'].quantile(0.7)
# foreign_thresh = 0.5

priority_areas = df_merge[
    (df_merge['ì¸êµ¬ë°€ë„'] >= pop_density_thresh) &
    (df_merge['ê³ ë ¹ììˆ˜'] >= elderly_thresh) ]
    # (df_merge['ì™¸êµ­ì¸ë¹„ìœ¨'] >= foreign_thresh)

# 7) ê²°ê³¼ ì¶œë ¥
print("ìš°ì„  ëŒ€ì‘ í•„ìš” ìë©´ë™:")
print(priority_areas[['í–‰ì •êµ¬', 'ìë©´ë™', 'ì¸êµ¬ë°€ë„', 'ê³ ë ¹ììˆ˜']])


```

## () ê³ ìœ„í—˜ top 10: ì¸êµ¬ë°€ë„40% + ê³ ë ¹ì40% + ì™¸êµ­ì¸20%

```{python}

# ê²°ì¸¡ì¹˜ ì²˜ë¦¬
df_merge['ê³ ë ¹ììˆ˜'] = df_merge['ê³ ë ¹ììˆ˜'].fillna(0)
df_merge['ì™¸êµ­ì¸ë¹„ìœ¨'] = df_merge['ì™¸êµ­ì¸ë¹„ìœ¨'].fillna(0)

df_merge.columns

# ì‚¬ìš©í•  ì»¬ëŸ¼ ì¶”ì¶œ
risk_df = df_merge[['í–‰ì •êµ¬', 'ìë©´ë™', 'ì¸êµ¬ë°€ë„', 'ê³ ë ¹ììˆ˜', 'ì™¸êµ­ì¸ë¹„ìœ¨']].copy()

# inf ê°’ â†’ NaN ë³€í™˜
risk_df[['ì¸êµ¬ë°€ë„', 'ê³ ë ¹ììˆ˜', 'ì™¸êµ­ì¸ë¹„ìœ¨']] = risk_df[['ì¸êµ¬ë°€ë„', 'ê³ ë ¹ììˆ˜', 'ì™¸êµ­ì¸ë¹„ìœ¨']].replace([np.inf, -np.inf], np.nan)

# NaN ì±„ìš°ê¸° (ì˜ˆ: 0ìœ¼ë¡œ ì±„ì›€, ë˜ëŠ” í‰ê· /ì¤‘ì•™ê°’ìœ¼ë¡œ)
risk_df = risk_df.fillna(0)


# ì •ê·œí™” (Min-Max scaling)
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
risk_df[['ì¸êµ¬ë°€ë„_norm', 'ê³ ë ¹ììˆ˜_norm', 'ì™¸êµ­ì¸ë¹„ìœ¨_norm']] = scaler.fit_transform(
    risk_df[['ì¸êµ¬ë°€ë„', 'ê³ ë ¹ììˆ˜', 'ì™¸êµ­ì¸ë¹„ìœ¨']])


# ê°€ì¤‘ì¹˜ ì§€ì •
weights = {
    'ì¸êµ¬ë°€ë„_norm': 0.4,
    'ê³ ë ¹ììˆ˜_norm': 0.4,
    'ì™¸êµ­ì¸ë¹„ìœ¨_norm': 0.2
}

# ìœ„í—˜ë„ ì ìˆ˜ ê³„ì‚°
risk_df['ìœ„í—˜ë„ì ìˆ˜'] = (
    risk_df['ì¸êµ¬ë°€ë„_norm'] * weights['ì¸êµ¬ë°€ë„_norm'] +
    risk_df['ê³ ë ¹ììˆ˜_norm'] * weights['ê³ ë ¹ììˆ˜_norm'] +
    risk_df['ì™¸êµ­ì¸ë¹„ìœ¨_norm'] * weights['ì™¸êµ­ì¸ë¹„ìœ¨_norm']
)

# ìœ„í—˜ë„ ì ìˆ˜ ìƒìœ„ 10ê°œ ì§€ì—­ ì¶œë ¥
top_risk_areas = risk_df.sort_values(by='ìœ„í—˜ë„ì ìˆ˜', ascending=False).head(10)

print(top_risk_areas[['í–‰ì •êµ¬', 'ìë©´ë™', 'ìœ„í—˜ë„ì ìˆ˜']])

```

# 3. ì§€ë¦¬ì  ìš”ì¸

```{python}
# ì§€ë„ ì „ì²˜ë¦¬


#########################################################################
# 1. í–‰ì •ë™ GeoJSON ë¶ˆëŸ¬ì˜¤ê¸°
# -----------------------------------------------------------------------
geo = gpd.read_file("./data/hangjeongdong_ëŒ€êµ¬ê´‘ì—­ì‹œ.geojson")
geo2 = gpd.read_file("./data/hangjeongdong_ê²½ìƒë¶ë„.geojson")

# êµ°ìœ„ ë°ì´í„° ì¶”ì¶œ
geo1 = geo2[geo2['adm_nm'].str.contains('ê²½ìƒë¶ë„ êµ°ìœ„', na=False)].copy()

# ëŒ€êµ¬ + êµ°ìœ„ í•©ì¹˜ê¸°
geo_merged = pd.concat([geo, geo1], ignore_index=True)
geo_merged = gpd.GeoDataFrame(geo_merged, crs=geo.crs)

#########################################################################
# 2. ì¸êµ¬ë°€ë„ CSV ë¶ˆëŸ¬ì˜¤ê¸°
# -----------------------------------------------------------------------
pop = dens
# 'í–‰ì •êµ¬'ì™€ 'ìë©´ë™'ì„ í•©ì³ 'adm_nm' ì„ì‹œ ìƒì„±
pop['adm_nm'] = pop['í–‰ì •êµ¬'] + " " + pop['ìë©´ë™']

#########################################################################
# âœ¨ [ìˆ˜ì • 1] ë°ì´í„° ë³‘í•© ì „, KEY ê°’(adm_nm) í†µì¼
# -----------------------------------------------------------------------
# 1. ì–‘ìª½ ë°ì´í„°ì˜ ë¶ˆí•„ìš”í•œ ì•ë’¤ ê³µë°± ì œê±°
geo_merged['adm_nm'] = geo_merged['adm_nm'].str.strip()
pop['adm_nm'] = pop['adm_nm'].str.strip()

# 2. GeoJSONì˜ 'ê²½ìƒë¶ë„ êµ°ìœ„êµ°'ì„ 'êµ°ìœ„êµ°'ìœ¼ë¡œ ë³€ê²½ (ëŒ€êµ¬ í¸ì… ë°˜ì˜)
geo_merged['adm_nm'] = geo_merged['adm_nm'].str.replace('ê²½ìƒë¶ë„ êµ°ìœ„êµ°', 'êµ°ìœ„êµ°', regex=False)

# 3. ì¸êµ¬ ë°ì´í„°ì˜ 'adm_nm'ì„ GeoJSON í˜•ì‹('ëŒ€êµ¬ê´‘ì—­ì‹œ OOêµ¬ OOë™')ê³¼ ì¼ì¹˜ì‹œí‚¤ê¸°
# êµ°ìœ„êµ°ì€ 'ëŒ€êµ¬ê´‘ì—­ì‹œ'ë¥¼ ë¶™ì´ì§€ ì•Šë„ë¡ ì˜ˆì™¸ ì²˜ë¦¬
is_gunwi = pop['í–‰ì •êµ¬'] == 'êµ°ìœ„êµ°'
pop.loc[~is_gunwi, 'adm_nm'] = 'ëŒ€êµ¬ê´‘ì—­ì‹œ ' + pop.loc[~is_gunwi, 'adm_nm']



```

## (1) ë¹—ë¬¼íŒí”„ì¥ ìœ„ì¹˜

```{python}

# ë¹—ë¬¼íŒí”„ì¥ ë°ì´í„°

# --- 1. ë°ì´í„° ë¡œë“œ ---
# í–‰ì •ë™ GeoJSON
geo_daegu = gpd.read_file(".\data\hangjeongdong_ëŒ€êµ¬ê´‘ì—­ì‹œ.geojson")
geo_gyeongbuk = gpd.read_file(".\data\hangjeongdong_ê²½ìƒë¶ë„.geojson")

# ë¹—ë¬¼íŒí”„ì¥ ì •ë³´ CSV (ì‰¼í‘œë¡œ êµ¬ë¶„ëœ íŒŒì¼)
pump_df = pd.read_csv('.\data\ë¹—ë¬¼íŒí”„ì¥final.csv', sep=',')

# --- 2. ë°ì´í„° ì „ì²˜ë¦¬ ---
# í–‰ì •ë™ ë°ì´í„° ì¤€ë¹„
geo_gunwi = geo_gyeongbuk[geo_gyeongbuk['adm_nm'].str.contains('ê²½ìƒë¶ë„ êµ°ìœ„êµ°', na=False)].copy()
geo_merged = pd.concat([geo_daegu, geo_gunwi], ignore_index=True)
geo_merged = gpd.GeoDataFrame(geo_merged, crs=geo_daegu.crs)
geo_merged['adm_nm'] = geo_merged['adm_nm'].str.strip().str.replace('ê²½ìƒë¶ë„ êµ°ìœ„êµ°', 'êµ°ìœ„êµ°', regex=False)

# íŒí”„ì¥ ë°ì´í„° ì¤€ë¹„
pump_df.dropna(subset=['ì„¤ì¹˜ë…„ë„', 'ìœ„ë„ (Latitude)', 'ê²½ë„ (Longitude)'], inplace=True)
pump_df['ì„¤ì¹˜ë…„ë„'] = pd.to_numeric(pump_df['ì„¤ì¹˜ë…„ë„'], errors='coerce')

# íŒí”„ì¥ ìœ„ì¹˜ ë°ì´í„°(GeoDataFrame) ìƒì„± ë° ì¢Œí‘œê³„ í†µì¼
pump_gdf = gpd.GeoDataFrame(
    pump_df,
    geometry=gpd.points_from_xy(pump_df['ê²½ë„ (Longitude)'], pump_df['ìœ„ë„ (Latitude)']),
    crs="EPSG:4326"
).to_crs(geo_merged.crs)

# ê³µê°„ ê²°í•©ìœ¼ë¡œ í–‰ì •ë™ë³„ íŒí”„ì¥ ì •ë³´ ì§‘ê³„
pump_join = gpd.sjoin(pump_gdf, geo_merged, how="left", predicate="within")

# ======================== âœ¨ í•µì‹¬ ìˆ˜ì • ë¶€ë¶„ âœ¨ ========================
# 1. í–‰ì •ë™ë³„ 'ê°€ì¥ ì˜¤ë˜ëœ ì„¤ì¹˜ë…„ë„' ì°¾ê¸°
oldest_pump = pump_join.groupby("adm_nm")['ì„¤ì¹˜ë…„ë„'].min().reset_index()
oldest_pump.rename(columns={'ì„¤ì¹˜ë…„ë„': 'oldest_pump_year'}, inplace=True)

# 2. í˜„ì¬ ì—°ë„ ê¸°ì¤€ 'ê²½ê³¼ë…„ìˆ˜'ë¥¼ ìœ„í—˜ ì ìˆ˜ë¡œ ë³€í™˜ (ì˜¤ë˜ë ìˆ˜ë¡ ì ìˆ˜ê°€ ë†’ìŒ)
current_year = pd.to_datetime('today').year
oldest_pump['risk_score'] = current_year - oldest_pump['oldest_pump_year']
# ======================================================================

# 3. í–‰ì •ë™ë³„ íŒí”„ì¥ ìƒì„¸ ì •ë³´ ë¬¸ìì—´ ìƒì„± (íˆ´íŒìš©)
def format_pump_details(df_group):
    details = [f"{row['íŒí”„ì¥ëª…']} ({int(row['ì„¤ì¹˜ë…„ë„'])}ë…„)" for idx, row in df_group.iterrows()]
    return '<br>'.join(details)
pump_details = pump_join.groupby("adm_nm").apply(format_pump_details).reset_index(name="pump_details")

# --- 3. ìµœì¢… ë°ì´í„° í†µí•© ---
df = geo_merged.merge(oldest_pump, on="adm_nm", how="left")
df = df.merge(pump_details, on="adm_nm", how="left")
df['risk_score'].fillna(0, inplace=True) # íŒí”„ì¥ ì—†ëŠ” ì§€ì—­ì€ 0ì 
df['pump_details'].fillna("íŒí”„ì¥ ì •ë³´ ì—†ìŒ", inplace=True)
df['oldest_pump_year'].fillna("ì—†ìŒ", inplace=True)


# --- 4. ì§€ë„ ì‹œê°í™” ---
m = folium.Map(location=[35.87, 128.6], zoom_start=11)

# [ì—­í•  1] ë°°ê²½ìƒ‰ Choropleth ë ˆì´ì–´ (ìœ„í—˜ ì ìˆ˜ ê¸°ì¤€)
folium.Choropleth(
    geo_data=df,
    data=df,
    columns=["adm_nm", "risk_score"],
    key_on="feature.properties.adm_nm",
    fill_color="YlOrRd",
    fill_opacity=0.7,
    line_opacity=0.5,
    legend_name="ê°€ì¥ ì˜¤ë˜ëœ íŒí”„ì¥ì˜ ê²½ê³¼ë…„ìˆ˜ (ë†’ì„ìˆ˜ë¡ ìœ„í—˜)"
).add_to(m)

# [ì—­í•  2] íˆ´íŒ, í…Œë‘ë¦¬ì„ , í•˜ì´ë¼ì´íŠ¸ GeoJson ë ˆì´ì–´
tooltip = folium.features.GeoJsonTooltip(
    fields=['adm_nm', 'risk_score', 'pump_details'],
    aliases=['<b>í–‰ì •ë™:</b>', '<b>ìµœê³  ê²½ê³¼ë…„ìˆ˜:</b>', '<b>ì„¤ì¹˜ëœ íŒí”„ì¥:</b>'],
    style=("background-color: white; color: #333333; font-family: arial; font-size: 12px; padding: 10px;"),
    sticky=True
)

folium.GeoJson(
    df,
    style_function=lambda x: {'fillColor': 'transparent', 'color': 'transparent', 'weight': 0.5},
    highlight_function=lambda x: {'color': 'black', 'weight': 2},
    tooltip=tooltip
).add_to(m)

folium.LayerControl().add_to(m)

# m.save("ëŒ€êµ¬_íŒí”„ì¥_ìµœê³ ë…¸í›„ë„_ì§€ë„.html")
# print("ì§€ë„ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤: ëŒ€êµ¬_íŒí”„ì¥_ìµœê³ ë…¸í›„ë„_ì§€ë„.html")


```

## (2) í•˜ì²œ ì¸ê·¼ ì§€ì—­

```{python}
# ì €ì§€ëŒ€ ê¸°ì¤€ ì ìš© -> ì €ì§€ëŒ€ ë„ì¶œ

# --- 1. ë°ì´í„° ë¡œë“œ ---
# ì¹¨ìˆ˜ ìœ„í—˜ ì§€ì—­ ëª©ë¡ CSV
hazard_df = pd.read_csv('.\data\ëŒ€êµ¬_ì¹¨ìˆ˜ìœ„í—˜ì§€ì—­_í•˜ì²œ í¬í•¨2_ì¢Œí‘œ.csv')

# í–‰ì •êµ¬ì—­ ê²½ê³„(GeoJSON) ë°ì´í„°
geo_daegu = gpd.read_file(".\data\hangjeongdong_ëŒ€êµ¬ê´‘ì—­ì‹œ.geojson")
geo_gyeongbuk = gpd.read_file(".\data\hangjeongdong_ê²½ìƒë¶ë„.geojson")


# --- 2. ë°ì´í„° ì¤€ë¹„ ---
# êµ°ìœ„êµ° ë°ì´í„°ë¥¼ ëŒ€êµ¬ì‹œì— ë³‘í•©
geo_gunwi = geo_gyeongbuk[geo_gyeongbuk['adm_nm'].str.contains('ê²½ìƒë¶ë„ êµ°ìœ„êµ°', na=False)].copy()
geo_merged = pd.concat([geo_daegu, geo_gunwi], ignore_index=True)
geo_merged = gpd.GeoDataFrame(geo_merged, crs=geo_daegu.crs)
geo_merged['adm_nm'] = geo_merged['adm_nm'].str.strip().str.replace('ê²½ìƒë¶ë„ êµ°ìœ„êµ°', 'êµ°ìœ„êµ°', regex=False)


# --- 3. ìœ„í—˜ ì ìˆ˜ ìƒì„± ---
# ìœ„í—˜ ì§€ì—­ ëª©ë¡ì— ìˆëŠ” í–‰ì •ë™ ì´ë¦„ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“­ë‹ˆë‹¤.
# 'ëŒ€êµ¬ê´‘ì—­ì‹œ'ë¥¼ ì¶”ê°€í•˜ì—¬ geo_mergedì˜ ì´ë¦„ í˜•ì‹ê³¼ í†µì¼í•©ë‹ˆë‹¤.
hazard_list = ['ëŒ€êµ¬ê´‘ì—­ì‹œ ' + name for name in hazard_df['í–‰ì •êµ¬ì—­']]
gunwi_list = [name for name in hazard_df['í–‰ì •êµ¬ì—­'] if 'êµ°ìœ„êµ°' in name]
hazard_list.extend(gunwi_list)


# geo_merged ë°ì´í„°í”„ë ˆì„ì— 'risk_score' ì»¬ëŸ¼ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
# ìœ„í—˜ ì§€ì—­ ëª©ë¡ì— í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ 1ì , ì•„ë‹ˆë©´ 0ì ì„ ë¶€ì—¬í•©ë‹ˆë‹¤.
geo_merged['risk_score'] = geo_merged['adm_nm'].apply(lambda x: 1 if x in hazard_list else 0)


# --- 4. ì§€ë„ ì‹œê°í™” ---
m = folium.Map(location=[35.8714, 128.6014], zoom_start=11)

# [ì—­í•  1] ë°°ê²½ìƒ‰ Choropleth ë ˆì´ì–´
folium.Choropleth(
    geo_data=geo_merged,
    data=geo_merged,
    columns=['adm_nm', 'risk_score'],
    key_on='feature.properties.adm_nm',
    fill_color='YlOrRd',
    fill_opacity=0.7,
    line_opacity=0.5,
    legend_name='ì¹¨ìˆ˜ ìœ„í—˜ë„ (1: ìœ„í—˜ ì§€ì—­)'
).add_to(m)

# [ì—­í•  2] íˆ´íŒ ë° ê²½ê³„ì„  í‘œì‹œ
tooltip = folium.features.GeoJsonTooltip(
    fields=['adm_nm'],
    aliases=['í–‰ì •ë™:'],
    style=("background-color: white; color: #333333; font-family: arial; font-size: 12px; padding: 10px;")
)

folium.GeoJson(
    geo_merged,
    style_function=lambda x: {
        'fillOpacity': 0,
        'color': 'black',
        'weight': 0.5
    },
    tooltip=tooltip
).add_to(m)


# --- 5. ì €ì¥ ---
folium.LayerControl().add_to(m)
# m.save('í•˜ì²œ ì¸ê·¼ ì§€ì—­_map.html')
# print("ğŸ‰ ì¹¨ìˆ˜ ìœ„í—˜ ì§€ì—­ì´ ìƒ‰ìƒìœ¼ë¡œ í‘œì‹œëœ ì§€ë„ê°€ 'í•˜ì²œ ì¸ê·¼ ì§€ì—­_map.html' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

```

# \[ê²°ë¡ \]

```{python}
# ë™ì¼ê°€ì¤‘ì¹˜ -> ì¢…í•©ì ìˆ˜ -> ì‹œê°í™”
# ê°€ì¤‘ì¹˜ëŠ” íƒ­ í•˜ë‚˜ í•´ì„œ ì¡°ì ˆ ê°€ëŠ¥í•˜ê²Œ

```

```{python}
# ìœ„í—˜ì ìˆ˜ ë†’ì€ì§€ì—­/ë‚®ì€ì§€ì—­ êµ¬ë¶„ -> ì§€ì—­ë³„ë¡œ ë¹„êµë¶„ì„
# ìœ„í—˜ë„ ì ìˆ˜ íˆìŠ¤í† ê·¸ë¨ -> ë‚®ì€ì§€ì—­, ë†’ì€ì§€ì—­ ë½‘ì•„ì„œ ë¹„êµ
# ë¹„êµ: ê° ì¸ìë³„ë¡œ ì–´ë–¤ íŠ¹ì„±ë•Œë¬¸ì— ì ìˆ˜ê°€ ë‚®/ë†’ê²Œ ë‚˜ì˜¤ëŠ”ì§€



# + ëŒ€êµ¬ì‹œ ì§€ë¦¬, ì¸êµ¬, ë‚ ì”¨ ì™¸ ë‹¤ë¥¸ ìš”ì¸ì´ ìˆì„ ìˆ˜ ìˆìœ¼ë‹ˆê¹Œ, ì¹¨ìˆ˜ ìœ„í—˜ì´ ë†’ì€ ì§€ì—­ ê¸°ì¤€ìœ¼ë¡œ
# ê¸°ì‚¬ ë“±ë“±ë“± ê¸°íƒ€ ì¤‘ìš” ìš”ì¸ ì°¾ì•„ë³´ê¸° -> í•´ê²°ì±… ì œì‹œ/ë„ì¶œ(í•´ê²°ì±…ì€ êµ¬ì²´ì ì´ì§€ ì•Šì•„ë„ ~~~ ë°©ì•ˆì´ ìˆì„ ê²ƒì´ë‹¤ ì œì‹œ)
```

## ê³¼ê±° ì¬ë‚œ ì§€ì—­ ëŒ€ë¹„

-\> (ê²°ë¡ ë‹¨ê³„ì—ì„œ ì‚¬ìš© ì˜ˆì •)

ë°ì´í„° ë¶„ì„ì„ í†µí•œ ìœ„í—˜ì§€ì—­ ë„ì¶œ í›„ ì‹¤ì œ ìœ„í—˜ì§€ì—­ê³¼ ëŒ€ë¹„ë¥¼ í†µí•œ ê²€ì¦ ë°ì´í„°ë¡œ í™œìš© ì˜ˆì •

```{python}
pd.read_csv('./data/ëŒ€êµ¬ê´‘ì—­ì‹œ_ì¬í•´ìœ„í—˜ì§€êµ¬_í†µí•©.csv', encoding='cp949')


# ì‹¤ì œ ì¹¨ìˆ˜ ë°ì´í„°ë‘ ë§¤ì¹­ì´ ë˜ëŠ”ì§€ ì ê²€ -> ë™ìœ¼ë¡œ ë¶„ì„ ì§„í–‰(ì˜ˆì •)
```